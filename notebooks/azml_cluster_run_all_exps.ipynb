{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a single experiment on an AzML Compute Cluster\n",
    "\n",
    "In this notebook we demonstrate running a single experiemnt (load data, train classifier, evaluate accuracy, produce classifications) running not locally on the same compute instance as the notebook, but rather being submitted to a compute cluster.\n",
    "\n",
    "To handle elements of  the processing pipeline that are different running in this way, I have created some classes derived from the main `XbtDataset` and `ClassificationExperiment` classes, namely `AzureDataset` and `AzureExperiment`. The principle changes are:\n",
    "\n",
    "* loading data from a mounted AzML Dataset\n",
    "* locating where the JSON experiment file has been copied to by the submit function that launches the experiment on the cluster.\n",
    "* registering results with the Run in the AzML Experiment Framework through the API.\n",
    "* (TODO) writing the output classifications to an Azure Blob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repo_dir = pathlib.Path().absolute().parent\n",
    "sys.path = [os.path.join(root_repo_dir)] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import azureml.core.compute\n",
    "import azureml.core.compute_target\n",
    "import azureml.train.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xbt.azureml\n",
    "import xbt.common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parameters\n",
    "Define some key paths for the experiment. Paths are not generally defined in experiment description, to make the experiment description more portable.\n",
    "Import definitions include\n",
    "* The root data directory. This should have subdirectories with the XBT input dataset, as well as for outputs.\n",
    "* The names of the input and output subdirectories\n",
    "* The path to JSON experiment description file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some site specific parameters for the notebook\n",
    "try:\n",
    "    environment = os.environ['XBT_ENV_NAME']\n",
    "except KeyError:\n",
    "    environment = 'azureml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AZURE ML SPECIFIC definitions\n",
    "azure_working_root = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad'\n",
    "xbt_compute_cluster_name = 'xbt-cluster'\n",
    "xbt_vm_size = 'STANDARD_D2_V2'\n",
    "xbt_max_nodes = 4\n",
    "\n",
    "# would be good if AzML could figure this from the user info / credentials, as I don't think a user can access other when logged into to a particular workspace?\n",
    "azml_subscription_id = '1fedcbc3-e156-45f5-a034-c89c2fc0ac61'\n",
    "azml_resource_group = 'AWSEarth'\n",
    "azml_workspace_name = 'stephenHaddad_xbt_europeWest'\n",
    "\n",
    "azml_xbt_dataset_name = 'xbt_input_files'\n",
    "azml_output_datastore_name = 'misc'\n",
    "azml_output_datastore_dir = 'xbt-data/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/',\n",
    "    'pangeo': '/data/misc/xbt-data/',\n",
    "    'azureml': os.path.join(azure_working_root, 'xbt-data'),\n",
    "}\n",
    "env_date_ranges = {\n",
    "    'MO_scitools': (1966,2015),\n",
    "    'pangeo': (1966,2015),\n",
    "    'azureml': (1966,2015),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some dataset specific parameters\n",
    "root_data_dir = root_data_dirs[environment]\n",
    "year_range = env_date_ranges[environment]\n",
    "exp_name_template = 'cluster_azml_{exp_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_name = 'csv_with_imeta'\n",
    "exp_out_dir_name = 'experiment_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_input_dir = os.path.join(root_data_dir, input_dir_name)\n",
    "xbt_output_dir = os.path.join(root_data_dir, exp_out_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_root_dir = os.path.join(root_repo_dir, 'experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_json_fname_list = [fn1 for fn1 in os.listdir(exp_root_dir) if os.path.isfile(os.path.join(exp_root_dir, fn1)) and '.json' in fn1]\n",
    "exp_json_path_list = [os.path.join(exp_root_dir, fn1) for fn1 in exp_json_fname_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Azure ML environment stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "location of working dir on the compute instance, not on the cluster. This will be spceific to the compute instance, so we need to find a better way to do this.\n",
    "It would be good if this sort of thing could be defined either through the AzML API, or in the compute instance environment variables, for example `$AZML_HOME_DIR`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_workspace = azureml.core.Workspace(azml_subscription_id, azml_resource_group, azml_workspace_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "We should create multiple experiments for each of the files, because each experiment file should correspond to an experiment on azureML and not be separate runs of the same experiemnt, given that so far experiments are different classifiers and input sets.\n",
    "\n",
    "* different experiments can be accessed in the loop through experiment definitions below\n",
    "* experiment names are derived from the experiment files\n",
    "* experiments are accessed, or created if they do not exist.\n",
    "* the run config is submitted to the relevant defintion, and the run object stored in a dict\n",
    "* runs can then be used for plotting results at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare/access the the Azure ML compute cluster\n",
    "If we want to use an AzureML clsuter for training, cross-validation, hyperparameter tuning etc. we need to create an object to access (and potentially start up) a suitable compute cluster.\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-10-22T23:25:09.340000+00:00', 'errors': None, 'creationTime': '2020-08-26T13:32:02.981314+00:00', 'modifiedTime': '2020-10-23T09:22:36.269768+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT1200S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D13_V2'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    compute_target = azureml.core.compute.ComputeTarget(workspace=xbt_workspace, name=xbt_compute_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except azureml.core.compute_target.ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = azureml.core.compute.AmlCompute.provisioning_configuration(vm_size=xbt_vm_size, \n",
    "                                                           max_nodes=xbt_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = azureml.core.compute.ComputeTarget.create(xbt_workspace, xbt_compute_cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on the cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_packages = ['python=3.8',\n",
    "                  'joblib=0.13.2',\n",
    "                  'pandas=1.0.1',\n",
    "                  'scikit-learn=0.22.1',\n",
    "                  'iris=2.4',\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_script = 'bin/run_azml_cv_experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "azml_xbt_dataset_obj = azureml.core.Dataset.get_by_name(xbt_workspace, azml_xbt_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_datastore = azureml.core.Datastore.get(xbt_workspace, azml_output_datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_datastore = azureml.core.Datastore.get_default(xbt_workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 12 files\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_country.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_countryLatLon.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_country_dev.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_latLon.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_maxDepthYear.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_knn_country.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_logreg_country.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_mlp_country.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_mlp_countryLatLon.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_randomForest_country.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_randomForest_countryLatLon.json\n",
      "Uploading /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_xgboost_countryLatLon.json\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_country.json, 1 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_randomForest_country.json, 2 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_xgboost_countryLatLon.json, 3 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_logreg_country.json, 4 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_country_dev.json, 5 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_knn_country.json, 6 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_countryLatLon.json, 7 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_maxDepthYear.json, 8 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_mlp_country.json, 9 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_decisionTree_latLon.json, 10 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_mlp_countryLatLon.json, 11 files out of an estimated total of 12\n",
      "Uploaded /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/XBTs_classification/experiments/xbt_param_randomForest_countryLatLon.json, 12 files out of an estimated total of 12\n",
      "Uploaded 12 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_941e759d125544b0b67f7040b5fa140d"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azml_datastore_exp_dir = 'exp_files'\n",
    "default_datastore.upload_files(exp_json_path_list, target_path=azml_datastore_exp_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--json-experiment': '',\n",
    "    '--input-dir': azml_xbt_dataset_obj.as_named_input(azml_xbt_dataset_name).as_mount(),\n",
    "    '--output-dir': 'xbt-data/azml_outputs',\n",
    "    '--config-dir': default_datastore.as_mount(), \n",
    "    '--output-root': xbt_datastore.as_mount(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_batch_runs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_decisionTree_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_decisionTree_countryLatLon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_decisionTree_country_dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_decisionTree_latLon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_decisionTree_maxDepthYear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_knn_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_logreg_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_mlp_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_mlp_countryLatLon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_RandomForest_country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_RandomForest_countryLatLon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting run for experiment cluster_azml_decisionTree_countryLatLon\n"
     ]
    }
   ],
   "source": [
    "for exp_fname, exp_path in zip(exp_json_fname_list, exp_json_path_list):\n",
    "    with open(exp_path) as exp_json_file:\n",
    "        exp_params = json.load(exp_json_file)    \n",
    "    experiment_name = exp_name_template.format(exp_name=exp_params['experiment_name'])\n",
    "    experiment = azureml.core.Experiment(workspace=xbt_workspace, name=experiment_name)\n",
    "    exp_json_azml_datasource_path = os.path.join(azml_datastore_exp_dir, exp_fname)\n",
    "    script_params['--json-experiment'] = os.path.join(azml_datastore_exp_dir, exp_fname)\n",
    "    xbt_estimator = azureml.train.sklearn.SKLearn(\n",
    "        source_directory=str(root_repo_dir), \n",
    "        script_params=script_params,\n",
    "        compute_target=compute_target,\n",
    "        entry_script=launch_script,\n",
    "        conda_packages=conda_packages,\n",
    "        )\n",
    "    print(f'submitting run for experiment {experiment_name}')\n",
    "    exp_batch_runs[experiment_name] = experiment.submit(xbt_estimator)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_batch_runs[list(exp_batch_runs.keys())[0]].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained classifier objects are saved to the output directory, one per file. There is also a JSON experiment description file, which is the same as the original description, but with inference added to experiment name and a list of classifier file names. This file can be used to create and run an inference job. The classifier files should be in the same directory as the JSON inference description files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_inf_path = exp2_cv.inference_out_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot results using metrics and scores files attched to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_results = matplotlib.pyplot.figure('xbt_results',figsize=(25,15))\n",
    "# for label1, metrics1  in classifiers_cv.items():\n",
    "#     ax_precision = fig_results.add_subplot(3,5,label1 +1, title='precision split {0}'.format(label1))\n",
    "#     ax_recall = fig_results.add_subplot(3,5,label1 + 1 + 5 * 1, title='recall split {0}'.format(label1))\n",
    "#     ax_f1 = fig_results.add_subplot(3,5,label1 + 1 + 5 * 2, title='f1 split {0}'.format(label1))\n",
    "#     results_cv.plot.line(ax=ax_precision, x='year', y=[f'precision_train_{label1}_all',f'precision_test_{label1}_all'], color=['b', 'r'], ylim=(0.7,1.0))\n",
    "#     results_cv.plot.line(ax=ax_recall, x='year', y=[f'recall_train_{label1}_all',f'recall_test_{label1}_all'], color=['b', 'r'], ylim=(0.7,1.0))\n",
    "#     results_cv.plot.line(ax=ax_f1, x='year', y=[f'f1_train_{label1}_all',f'f1_test_{label1}_all'], color=['b', 'r'], ylim=(0.7,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Once we have trained the classifiers, we want to be able to load from the saved state files and run inference on the whole dataset, with the same results. We can use the JSOn file created by the training to run the inference. The JSON inference parameters and the saved classifier object files should be in the same directory. We can then use the `run_inference` function. This will perform the following steps:\n",
    "* load dataset\n",
    "* load previously classifiers from file list defined in JSON inference description.\n",
    "* run inference for each of the classifiers\n",
    "* fill in classifications where not possible with classifiers using iMeta algorithm\n",
    "* calculate vote-based probability using ensemble of previously trained classifiers\n",
    "* save classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp3_inf = experiment.ClassificationExperiment(json_inf_path, xbt_input_dir, xbt_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# classifiers_reloaded = exp3_inf.run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
