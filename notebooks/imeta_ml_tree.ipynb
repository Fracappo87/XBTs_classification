{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A machine learning decision tree approach\n",
    "\n",
    "The iMeta algorithm is essentially a decision tree algorithm, where the variables and threshold for the decisions at each step are manually specified based on human analysis. The simplest way to apply machine learning techniques to the problem would be to use a similar structure to iMeta, which is a decision tree, but use standard ML training techiniques to learn the parameters such as what thresholds to use and how many branches/leaves to have in the tree for the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repo_dir = pathlib.Path().absolute().parent\n",
    "sys.path = [os.path.join(root_repo_dir)] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xbt.dataset\n",
    "from xbt.dataset import XbtDataset, UNKNOWN_STR, cat_output_formatter, check_value_found\n",
    "from xbt.imeta import imeta_classification, XBT_MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some site specific parameters for the notebook\n",
    "try:\n",
    "    environment = os.environ['XBT_ENV_NAME']\n",
    "except KeyError:\n",
    "    environment = 'pangeo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/',\n",
    "    'pangeo': '/data/misc/xbt-data/',\n",
    "}\n",
    "env_date_ranges = {\n",
    "    'MO_scitools': (1966,2015),\n",
    "    'pangeo': (1966,2015)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some dataset specific parameters\n",
    "root_data_dir = root_data_dirs[environment]\n",
    "year_range = env_date_ranges[environment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'nb_single_decisionTree_country'\n",
    "classifier_class = sklearn.tree.DecisionTreeClassifier\n",
    "classifier_name = 'decision_tree'\n",
    "suffix='country'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classifier_opts` dictionary contains the hyperparameters for the algorithm. For the decision tree, these primarily determine how deep the the tree becomes and how readily leaf nodes are split during the training process. The more splitting and the deeper the tree, the complicated the function in parameter space the tree is creating, but the greater the danger of overfitting.\n",
    "\n",
    "The criterion is the loss function used to optimise in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_opts = {'max_depth': 20,\n",
    "                   'min_samples_leaf': 1,\n",
    "                   'criterion': 'gini'\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metric_names = ['f1_weighted','precision_weighted','recall_weighted']\n",
    "input_feature_names = ['country','max_depth', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_name = 'csv_with_imeta'\n",
    "exp_out_dir_name = 'experiment_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_input_dir = os.path.join(root_data_dir, input_dir_name)\n",
    "xbt_output_dir = os.path.join(root_data_dir, exp_out_dir_name, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output for this experiment if it doesn't exist\n",
    "if not os.path.isdir(xbt_output_dir):\n",
    "    os.makedirs(xbt_output_dir)\n",
    "print(f'outputting to {xbt_output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname_template = 'xbt_output_{classifier}_{suffix}.csv'\n",
    "result_fname_template = 'xbt_metrics_{classifier}_{suffix}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "For the purposes of developing and evaluating the machine learning pipeline, we are using the labelled data, that is those profiles where the instrument type is known. Ultimately though it is the unknown data that we want to predict for.\n",
    "\n",
    "The XbtDataset class has filtered out some bad data including profiles with maximum depths less that 0.0 or greater than 2000.0. There were also some profiles with bad date entries, which have been excluded for now.\n",
    "\n",
    "Our training and evaluation will all happen within the labelled dataset, but ultimately we want to apply the trained ML algorithms to the unlabelled data, as this is where labels are needed to reduce the uncertainty in the ocean temperature dataset. So we need to design our experiment with this in mind. What we need to know is how well does our chosen algorithm generalise to unseen data. \n",
    "\n",
    "So within the labelled dataset we split the data into three groups. Firstly, we set a side some fraction of cruises and all profiles that are from those cruises into a group called \"unseen\". These profiles will not be used in the training or initial evaluation. These will represent cruises that have no labelled data. \n",
    "\n",
    "The remainder of the data will then be split into 2 groups, training and test as is standard for a machine learning pipeline. In splitting the data, we will sample a certain fraction of the profiles for train and the rest will be test. But we don't sample from the whole dataset randomly, as this might result in imbalances in representations of years and instrment types. Rather, for each combination of year and instrument label, sample the required fraction from all profiles matching those values. Doing this all possisble value combination of those 2 parameters should results in all years and instrument type being as well represented as possible in both the test and train sets.\n",
    "\n",
    "So the algorithm will be fit to the training set. Then we will evaulate performance on the test set. This gives us an idea of how the algorithm will perform on unlabelled profiles where other profiles from the same cruise are labelled. Then we will evaluate performance of the algorithm on the unseen dataset. This suggests how the algorithm will perform on unlabelled profiles where no profiles from that cruise are labelled. Both of these sorts of profiles have been found to be present in the unlabelled data, so it is important to evaluate the algorithm for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_full_dataset = XbtDataset(xbt_input_dir, year_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_labelled = xbt_full_dataset.filter_obs({'labelled': 'labelled'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the data in the pandas dataframe is not suitable for feeding into the machine learning algorithms. We have to appropriately encode the data for training. The dataset class has knowledge of the different types of data represented and how each type should be encoded. This happens through the get_ml_dataset method. Calling this with the return_data flag as False doesn't actually encode the data, but initialises the encoder objects required. These encoders are then used for subsequent encoding operations, so encoding are consistent for the whole experiment and results can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.get_ml_dataset(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.filter_features(['instrument', 'model', 'manufacturer']).encode_target(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unseen_cruise_numbers = xbt_labelled.sample_feature_values('cruise_number', fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_unseen = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='include', check_type='in_filter_set')\n",
    "xbt_working = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='exclude', check_type='in_filter_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imeta_classes = xbt_labelled.xbt_df.apply(imeta_classification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_model = imeta_classes.apply(lambda t1: t1[0])\n",
    "imeta_manufacturer = imeta_classes.apply(lambda t1: t1[1])\n",
    "imeta_instrument = imeta_classes.apply(lambda t1: f'XBT: {t1[0]} ({t1[1]})') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are initially also interested in investigating classifier performance looking at prove model and manufacturer separately, as well as looking at the combined model and manufacturer info as a single label and using that as the target feature.\n",
    "\n",
    "When splitting the data into test and train sets, we want to ensure that there is an even distribution of years and instrument types in train and test sets. In the train/test split function  used here, we divide the data into years and within the year divide it into instrument types, and then randomly sample from each group. This ensure even represenation of years and instruments. There is also a notebook where the splitting is done randomly with no accounting for class imbalances. The results are very similar, suggesting accuracy is not as sensitive to the precise split as it might seem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_train_all, xbt_test_all = xbt_working.train_test_split(refresh=True, features=['instrument', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = xbt_train_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_test_all = xbt_test_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_unseen_all = xbt_unseen.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "y_model_train_all = xbt_train_all.filter_features(['model']).get_ml_dataset()[0]\n",
    "y_manuf_train_all = xbt_train_all.filter_features(['manufacturer']).get_ml_dataset()[0]\n",
    "y_instr_train_all = xbt_train_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_model_test_all = xbt_test_all.filter_features(['model']).get_ml_dataset()[0]\n",
    "y_manuf_test_all = xbt_test_all.filter_features(['manufacturer']).get_ml_dataset()[0]\n",
    "y_instr_test_all = xbt_test_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_model_unseen_all = xbt_unseen.filter_features(['model']).get_ml_dataset()[0]\n",
    "y_manuf_unseen_all = xbt_unseen.filter_features(['manufacturer']).get_ml_dataset()[0]\n",
    "y_instr_unseen_all = xbt_unseen.filter_features(['instrument']).get_ml_dataset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier\n",
    "\n",
    "We are using the scikit-learn Decision Tree classifier as the closest analogue to the structure of the iMeta algorithm. This tree can have many more nodes and leaves than iMeta though. it is quick to train and evaluate so it is a useful starting point for setting up the ML processing pipelines, as all the scikit-learn classifiers have a common interface. \n",
    "\n",
    "For each of the target variables (probe model, probe manufacturer, combined probe model/manufacteer label), we train a Decision Tree classifier, then use it to predict values for the train and test sets. We then calculate the accuracy metrics for each for the whole dataset. \n",
    "\n",
    "I am using precision, recall and F1 as fairly standard ML metrics of accuracy. Recall is what has been used in the two previous papers (Palmer et. al, Leahy and Llopis et al) so that is the focus. Support is a useful to see what proportion of the profiles in the dataset belong to each of the different classes.\n",
    "\n",
    "Documentation\n",
    "* https://scikit-learn.org/stable/modules/tree.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all = {}\n",
    "metrics_avg_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all['model'] = list(xbt_labelled._feature_encoders['model'].classes_)\n",
    "metrics_per_class_all['manufacturer'] = list(xbt_labelled._feature_encoders['manufacturer'].classes_)\n",
    "metrics_per_class_all['instrument'] = list(xbt_labelled._feature_encoders['instrument'].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_dt_model1 = classifier_class(**classifier_opts)\n",
    "clf_dt_model1.fit(X_train_all,y_model_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_train_model_all = clf_dt_model1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_model_train_all, y_res_train_model_all,  labels=list(range(0,len(metrics_per_class_all['model']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_model_train': metrics1[0],\n",
    "    'recall_model_train': metrics1[1],\n",
    "    'f1_model_train': metrics1[2],\n",
    "    'support_model_train': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_model_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_model_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_model_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_model = clf_dt_model1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_model_test_all, y_res_test_model,  labels=list(range(0,len(metrics_per_class_all['model']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_model_test': metrics1[0],\n",
    "    'recall_model_test': metrics1[1],\n",
    "    'f1_model_test': metrics1[2],\n",
    "    'support_model_test': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_model_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_model_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_model_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_unseen_model = clf_dt_model1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_model_unseen_all, y_res_unseen_model, labels=list(range(0,len(metrics_per_class_all['model']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_model_unseen': metrics1[0],\n",
    "    'recall_model_unseen': metrics1[1],\n",
    "    'f1_model_unseen': metrics1[2],\n",
    "    'support_model_unseen': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_model_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_model_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_model_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_manuf1 = classifier_class(**classifier_opts)\n",
    "clf_dt_manuf1.fit(X_train_all,y_manuf_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_train_manuf_all = clf_dt_manuf1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_manuf_train_all, y_res_train_manuf_all, labels=list(range(0,len(metrics_per_class_all['manufacturer']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_manuf_train': metrics1[0],\n",
    "    'recall_manuf_train': metrics1[1],\n",
    "    'f1_manuf_train': metrics1[2],\n",
    "    'support_manuf_train': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_manuf_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_manuf_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_manuf_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_manuf_all = clf_dt_manuf1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_manuf_test_all, y_res_test_manuf_all, labels=list(range(0,len(metrics_per_class_all['manufacturer']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_manuf_test': metrics1[0],\n",
    "    'recall_manuf_test': metrics1[1],\n",
    "    'f1_manuf_test': metrics1[2],\n",
    "    'support_manuf_test': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_manuf_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_manuf_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_manuf_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_unseen_manuf_all = clf_dt_manuf1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_manuf_unseen_all, y_res_unseen_manuf_all, labels=list(range(0,len(metrics_per_class_all['manufacturer']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_manuf_unseen': metrics1[0],\n",
    "    'recall_manuf_unseen': metrics1[1],\n",
    "    'f1_manuf_unseen': metrics1[2],\n",
    "    'support_manuf_unseen': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_manuf_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_manuf_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_manuf_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_instr1 = classifier_class(**classifier_opts)\n",
    "clf_dt_instr1.fit(X_train_all,y_instr_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_res_train_instr_all = clf_dt_instr1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_train_all, y_res_train_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_train': metrics1[0],\n",
    "    'recall_instr_train': metrics1[1],\n",
    "    'f1_instr_train': metrics1[2],\n",
    "    'support_instr_train': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_res_test_instr_all = clf_dt_instr1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_test_all, y_res_test_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_test': metrics1[0],\n",
    "    'recall_instr_test': metrics1[1],\n",
    "    'f1_instr_test': metrics1[2],\n",
    "    'support_instr_test': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_res_unseen_instr_all = clf_dt_instr1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_unseen_all, y_res_unseen_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_unseen': metrics1[0],\n",
    "    'recall_instr_unseen': metrics1[1],\n",
    "    'f1_instr_unseen': metrics1[2],\n",
    "    'support_instr_unseen': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_per_class_model = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'model' in k1})\n",
    "df_metrics_per_class_manuf = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'manuf' in k1})\n",
    "df_metrics_per_class_instr = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'instr' in k1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg = pandas.DataFrame.from_dict({\n",
    "    'target': ['model_train','model_test', 'model_unseen', 'manufacturer_train','manufacturer_test', 'manufacturer_unseen','instrument_train','instrument_test', 'instrument_unseen'],\n",
    "    'precision': [v1 for k1,v1 in metrics_avg_all.items() if 'precision' in k1],\n",
    "    'recall': [v1 for k1,v1 in metrics_avg_all.items() if 'recall' in k1],\n",
    "    'f1': [v1 for k1,v1 in metrics_avg_all.items() if 'f1' in k1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification result plots\n",
    "\n",
    "The plots below show the results for the whole XBT dataset. We see that the DT classifier performs almost perfectly on the training data, but performance degrades for the test data, and further still for the profiles from unseen cruises. So it does not seem to generalise that well, but results are still better than other algorithms where the train and test results are closer together. Performance is poor for classes with very little support in the training dataset, as one might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_per_class_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_results_all_dt = matplotlib.pyplot.figure('xbt_results_all_dt', figsize=(24,24))\n",
    "axis_model_metrics = fig_results_all_dt.add_subplot(321)\n",
    "_ = df_metrics_per_class_model.plot.bar(x='model',y=['recall_model_train', 'recall_model_test', 'recall_model_unseen'], ax=axis_model_metrics)\n",
    "axis_model_support = fig_results_all_dt.add_subplot(322)\n",
    "_ = df_metrics_per_class_model.plot.bar(x='model',y=['support_model_train', 'support_model_test', 'support_model_unseen'], ax=axis_model_support)\n",
    "axis_manuf_metrics = fig_results_all_dt.add_subplot(323)\n",
    "_ = df_metrics_per_class_manuf.plot.bar(x='manufacturer', y=['recall_manuf_train','recall_manuf_test','recall_manuf_unseen'],ax=axis_manuf_metrics)\n",
    "axis_manuf_support = fig_results_all_dt.add_subplot(324)\n",
    "_ = df_metrics_per_class_manuf.plot.bar(x='manufacturer',y=['support_manuf_train', 'support_manuf_test', 'support_manuf_unseen'], ax=axis_manuf_support)\n",
    "axis_instr_metrics = fig_results_all_dt.add_subplot(325)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument', y=['recall_instr_train','recall_instr_test','recall_instr_unseen'],ax=axis_instr_metrics)\n",
    "axis_instr_support = fig_results_all_dt.add_subplot(326)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument',y=['support_instr_train', 'support_instr_test', 'support_instr_unseen'], ax=axis_instr_support)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of performance per class show that results generalise best where there are a substantial number of profiles of that probe type in the training and test sets. Further work may be needed to consider how to boost the performance of underepresented classes. Although these may be only a small number of profiles, we should also consider whether the measurements they contribute are in under represented areas and thus may have a disproportionately alrge impact on the final teprature values in some parts of the world in some years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg.plot.bar(figsize=(18,12), x='target', y='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "The contents of the XBT dataset varies over the time period, so previous papers have looked at classification accuracy (recall) year by year to evaluate how performance varies with different distribution of probe types.\n",
    "\n",
    "To do this we apply the classifier to the train, test  and unseen profile groups for each year separately. For each year we calculate the target metrics for each taret variable and plot the results. We compare the results to those achieved by the iMeta algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_year(xbt_df, year, clf, input_features, target_feature):\n",
    "    X_year = xbt_df.filter_obs({'year': year}, ).filter_features(input_features).get_ml_dataset()[0]\n",
    "    y_year = xbt_df.filter_obs({'year': year} ).filter_features([target_feature]).get_ml_dataset()[0]\n",
    "    y_res_year = clf.predict(X_year)\n",
    "    metric_year = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_year, y_res_year, average='micro')\n",
    "    return metric_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress = ipywidgets.IntProgress(min=env_date_ranges[environment][0],\n",
    "                                           max= env_date_ranges[environment][1],\n",
    "                                          description='Evaluating',\n",
    "                                          bar_style='info')\n",
    "eval_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    results_by_year[year] = {\n",
    "        'metric_train_model' : score_year(xbt_train_all, year, clf_dt_model1, input_feature_names, 'model'),\n",
    "        'metric_test_model' : score_year(xbt_test_all, year, clf_dt_model1, input_feature_names, 'model'),\n",
    "        'metric_unseen_model' : score_year(xbt_unseen, year, clf_dt_model1, input_feature_names, 'model'),\n",
    "        'metric_train_manuf' : score_year(xbt_train_all, year, clf_dt_manuf1, input_feature_names, 'manufacturer'),\n",
    "        'metric_test_manuf' : score_year(xbt_test_all, year, clf_dt_manuf1, input_feature_names, 'manufacturer'),\n",
    "        'metric_unseen_manuf' : score_year(xbt_unseen, year, clf_dt_manuf1, input_feature_names, 'manufacturer'),\n",
    "        'metric_train_instr' : score_year(xbt_train_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_test_instr' : score_year(xbt_test_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_unseen_instr' : score_year(xbt_unseen, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "    }\n",
    "    eval_progress.value = year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_by_year = pandas.DataFrame.from_dict({ \n",
    "    'year':  list(results_by_year.keys()),\n",
    "    'recall_train_model' : [m1['metric_train_model'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_model' : [m1['metric_test_model'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_model' : [m1['metric_unseen_model'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_train_manuf' : [m1['metric_train_manuf'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_manuf' : [m1['metric_test_manuf'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_manuf' : [m1['metric_unseen_manuf'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_train_instr' : [m1['metric_train_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_instr' : [m1['metric_test_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_instr' : [m1['metric_unseen_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = xbt_labelled._feature_encoders['model']\n",
    "manuf_encoder = xbt_labelled._feature_encoders['manufacturer']\n",
    "instr_encoder = xbt_labelled._feature_encoders['instrument']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_progress.value = env_date_ranges[environment][0]\n",
    "imeta_results = []\n",
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    y_imeta_model = model_encoder.transform(pandas.DataFrame(imeta_model[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_model1 = model_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].model))\n",
    "\n",
    "    y_imeta_manuf = manuf_encoder.transform(pandas.DataFrame(imeta_manufacturer[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_manufacturer1 = manuf_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].manufacturer))\n",
    "\n",
    "    y_imeta_instr = instr_encoder.transform(pandas.DataFrame(imeta_instrument[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_instr1 = instr_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].instrument))\n",
    "    \n",
    "    \n",
    "    (im_pr_model, im_rec_model, im_f1_model, im_sup_model) = sklearn.metrics.precision_recall_fscore_support(xbt_model1, y_imeta_model,average='micro')\n",
    "    (im_pr_manuf, im_rec_manuf, im_f1_manuf, im_sup_manuf) = sklearn.metrics.precision_recall_fscore_support(xbt_manufacturer1, y_imeta_manuf,average='micro')\n",
    "    (im_pr_instr, im_rec_instr, im_f1_instr, im_sup_instr) = sklearn.metrics.precision_recall_fscore_support(xbt_instr1, y_imeta_instr,average='micro')\n",
    "\n",
    "    imeta_results += [{'year': year,\n",
    "                       'imeta_model_recall': im_rec_model,\n",
    "                       'imeta_model_precision': im_pr_model,\n",
    "                       'imeta_manuf_recall': im_rec_manuf,\n",
    "                       'imeta_manuf_precision': im_pr_manuf,\n",
    "                       'imeta_instr_recall': im_rec_instr,\n",
    "                       'imeta_instr_precision': im_pr_instr,\n",
    "                      }]\n",
    "    eval_progress.value = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_res_df = pandas.DataFrame.from_records(imeta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pandas.merge(recall_by_year, imeta_res_df).merge(\n",
    "    pandas.DataFrame.from_dict({\n",
    "        'year': xbt_labelled['year'].value_counts(sort=False).index,\n",
    "        'num_samples': xbt_labelled['year'].value_counts(sort=False).values,\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_model_recall_results = matplotlib.pyplot.figure('xbt_model_recall', figsize=(18,6))\n",
    "ax_model_recall_results = fig_model_recall_results.add_subplot(131, title='XBT model recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_model','recall_test_model', 'recall_unseen_model', 'imeta_model_recall'], ax=ax_model_recall_results)\n",
    "ax_manuf_recall_results = fig_model_recall_results.add_subplot(132, title='XBT manufacturer recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_manuf','recall_test_manuf', 'recall_unseen_manuf', 'imeta_manuf_recall'], ax=ax_manuf_recall_results)\n",
    "ax_instr_recall_results = fig_model_recall_results.add_subplot(133, title='XBT instrument recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_instr','recall_test_instr', 'recall_unseen_instr', 'imeta_instr_recall'], ax=ax_instr_recall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for both the test and unseen profiles, the decision tree performs at least as well as the iMeta algorithm, and substantially better for some years, particularly the 1985-1995 era when there are a lot of profiles and a lot of them are missing probe type meta data. We can see how the algorithm genalises better in some years that others. The performance on the unseen profiles is slightly worse than the test profiles, but roughly comparable except in a few years where performance is substantially worse. This merits further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['improvement_model'] = results_df.apply(lambda r1: ((r1['recall_test_model'] /  r1['imeta_model_recall'])-1)*100.0 , axis=1)\n",
    "results_df['improvement_manuf'] = results_df.apply(lambda r1: ((r1['recall_test_manuf'] /  r1['imeta_manuf_recall'])-1)*100.0 , axis=1)\n",
    "results_df['improvement_instr'] = results_df.apply(lambda r1: ((r1['recall_test_instr'] /  r1['imeta_instr_recall'])-1)*100.0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num_samples_per_year = matplotlib.pyplot.figure('fig_num_samples_per_year', figsize=(16,16))\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(221, title='number of samples per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['num_samples'],c='purple' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(222, title='improvement model per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_model'], c='green' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(223, title='improvement manufacturer per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_manuf'], c='green' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(224, title='improvement instrument per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_instr'], c='green' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see substantial improvements in recall some years, particularly the early 1990s. This is a postive outcome as this where the high percent of unlabelled data is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(os.path.join(xbt_output_dir, result_fname_template.format(classifier=classifier_name,\n",
    "                                                                            suffix=suffix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing the data\n",
    "\n",
    "The output format of the probe classifications is both as a string label, as as a one hot encoding (OHE). OHE econdes the results as as nxm array with each of n rows representing a profile and each of m columns a profile type, with zeroes in most entries except a 1 in the column of the probe type for that profile. This is used as it can be genealised to providing probabilities, where instead of just ones and zeroes, each column represents the probability of the profile being from a particular probe type, with values between 0 and 1 that sum to 1 for each row. This will be explored further in subsequent notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checker functions check each element of the profile metadata that could be a problem. The checkers are constructed from the labelled data subset.\n",
    "checkers_labelled = {f1: c1 for f1, c1 in xbt_labelled.get_checkers().items() if f1 in input_feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_feature_name = 'instrument_res_dt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter based on what profiles we can use for predicting, we need some way of checking each profile we create these checkers from the labelled dataset, because the subset of data that was used for training determines what subset is valid for prediction. For example, if a country is not present in the training data, then the prediction function won't be able to handle that profile to predict a probe model and manufacturer. Profiles that are not handled by the trained classifier will get the label \"UNKNOWN\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_predictable = xbt_full_dataset.filter_predictable(checkers_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ml1 = clf_dt_instr1.predict(xbt_predictable.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "res2 = list(xbt_labelled._feature_encoders['instrument'].inverse_transform(res_ml1).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable.xbt_df[result_feature_name] = res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.merge_features(xbt_predictable, [result_feature_name],\n",
    "                               fill_values = {result_feature_name: UNKNOWN_STR},\n",
    "                               feature_encoders={result_feature_name: xbt_labelled._feature_encoders['instrument']},\n",
    "                                target_encoders={result_feature_name: xbt_labelled._target_encoders['instrument']},\n",
    "                               output_formatters={result_feature_name: [cat_output_formatter]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.output_data(out_dir=xbt_output_dir,\n",
    "                             fname_template=output_fname_template,\n",
    "                             exp_name=experiment_name,\n",
    "                             output_split=xbt.common.OUTPUT_SINGLE,\n",
    "                             target_features=[result_feature_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We can see so far that the basic tree xbt_output_dirch seems to be outperforming iMeta. Further work needs to be done to calculate these results more rigorously  using cross validation. \n",
    "\n",
    "This notebooks shows the basic classification pipeline. In the following notebooks, this will be used in the following ways\n",
    "* trying other machine learning algorithms\n",
    "* trying other input combinations\n",
    "* do hyper parameter tuning to find the best hyperparameters\n",
    "* cross-validation to check that the results are not anomolously good or bad based on the particular train/test split\n",
    "* run through all the different combinations and compare recall values to choose a best algorithm and input feature set.\n",
    "\n",
    "In addition, this pipeline will ultimately need to go into a script to do the training to be used by the ocean scientists to produce classifications that feed into the ocean datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XBT test 20200128",
   "language": "python",
   "name": "xbt_test_20200128"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
