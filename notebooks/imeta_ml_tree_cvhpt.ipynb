{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for decision trees\n",
    "\n",
    "We have now found that a decision tree, inspired by the iMeta algorithm, gives good results with the XBT dataset, and that the results hold up well using cross-validation, which shows that the initial results were not an artifact of the particular choice of split. The next step is to consider the hyperparameters of this algorithm. This affects whether the algorithm has a tendency to overfit or underfit. The optimal values for the hyperparameters tend to be problem dependent, so need to be tuned for each problem.\n",
    "\n",
    "Hyperparameter tuning trains the algorithm with different set of hyperparameter values, evaluates accruacy for erach and compares results to find the best hyperparameter combination for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tools\n",
    "Start by importing libraries and specifying the parameters for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repo_dir = pathlib.Path().absolute().parent\n",
    "sys.path = [os.path.join(root_repo_dir)] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xbt.dataset\n",
    "from xbt.dataset import XbtDataset, UNKNOWN_STR, cat_output_formatter, check_value_found\n",
    "from xbt.imeta import imeta_classification, XBT_MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbt.imeta import imeta_classification, XBT_MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some site specific parameters for the notebook\n",
    "try:\n",
    "    environment = os.environ['XBT_ENV_NAME']\n",
    "except KeyError:\n",
    "    environment = 'pangeo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/',\n",
    "    'pangeo': '/data/misc/xbt-data/',\n",
    "}\n",
    "env_date_ranges = {\n",
    "    'MO_scitools': (1966,2015),\n",
    "    'pangeo': (1966,2015)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define experiment parameters\n",
    "These are some key hyperparameters we're using for this experiment, such as input features and number of splits. These are hyperparameters that are no problem specific and are not part of the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'nb_decisionTree_cv_country'\n",
    "suffix='country'\n",
    "cv_metrics = ['recall_micro', 'precision_micro', 'f1_micro']\n",
    "hp_tune_metric = 'recall_micro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_names = ['country','max_depth', 'year']\n",
    "target_feature = 'instrument'\n",
    "splitter_features = ['year', 'instrument']\n",
    "unseen_feature = 'cruise_number'\n",
    "ensemble_unseen_fraction = 0.1\n",
    "num_unseen_splits = 5\n",
    "unseen_fraction = 1.0 / num_unseen_splits\n",
    "num_train_splits = 5\n",
    "train_fraction = 1.0 - (1.0 / num_train_splits)\n",
    "unseen_fold_name = 'unseen_fold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we define which classification algorithm we are using (a decision tree), and then we define the hyperparameter combination to be used in tuning. We are using grid search, so the tuner will create a list of all possible combination of the values of the three hyperparameters and try each to get the best result. The three hyperparameters we are tuning here are:\n",
    "* `max_depth` -  controls how deep the tree can get. The deeper the tree, the more complex decision function that can be created but the greater the danger of overfitting and poor generalisation. `None` in this case means the tree has no depth limit.\n",
    "  * *NOTE: this is the depth of the decision tree, not the temperature probe in this case!!*\n",
    "* `min_samples_leaf` trhis controls how the leaf nodes of the tree are split to model more complex decisions. Like with max depth, the morereadily the tree splits, the complex the decision can be, but the greater the danger of overfitting.\n",
    "* `criterion` - the loss function used in training the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_class = sklearn.tree.DecisionTreeClassifier\n",
    "classifier_opts = {}\n",
    "classifier_name = 'decision_tree_cv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_param_grid = {\n",
    "    'max_depth': [None, 1,5,10,20], \n",
    "    'min_samples_leaf': [1,2,5], \n",
    "    'criterion': ['gini', 'entropy']\n",
    "},\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some dataset specific parameters\n",
    "root_data_dir = root_data_dirs[environment]\n",
    "year_range = env_date_ranges[environment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_name = 'csv_with_imeta'\n",
    "exp_out_dir_name = 'experiment_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_input_dir = os.path.join(root_data_dir, input_dir_name)\n",
    "xbt_output_dir = os.path.join(root_data_dir, exp_out_dir_name, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting to /data/users/shaddad/xbt-data/experiment_outputs/nb_decisionTree_cv_country\n"
     ]
    }
   ],
   "source": [
    "# create the output for this experiment if it doesn't exist\n",
    "if not os.path.isdir(xbt_output_dir):\n",
    "    os.makedirs(xbt_output_dir)\n",
    "print(f'outputting to {xbt_output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname_template = 'xbt_output_{classifier}_{suffix}.csv'\n",
    "result_fname_template = 'xbt_metrics_{classifier}_{suffix}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the cross validation experiment\n",
    "\n",
    "In this experiment we will be combining hyperparameter tuning with cross-validation. There are multiple levels to the experiment we want to run. The first to train on part of the data and then test on another part. We want to divide up the profiles by cruise, so the test data consists of cruises that were not seen in the training data. We then want to train a model for each of the possible train/test splits. We divide the model into \"folds\", and then each splits uses data in fold n as the test set and ther rest as the train, as is standard in machine learning. In this case calculate the fold labels manually rather than using standard functions so we can divide based on cruise number and assign profiles to a fold based on which cruise they come from.\n",
    "\n",
    "For the training on eeach split, we also then want to use the best hyperparameters. So for each split we run a hyperparameter tuning scheme, in this case a grid search through the request values of the hyper parameters, decribed previously.\n",
    "\n",
    "The bottom level is that for each split and set of hyperparameters, we also want to run cross valdation within that training set, to check we have robust results. So we haveter and inner cross validation run together with hper parameter tuning to ensure we have robust results that are as good as we can get, and that demonstrate how well the trainined algorithm will generalise to the unlabelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on the approach taken here and the spceific classes and function used can found in the following links:\n",
    "\n",
    "scikit-learn API documentation:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "discussion of cross-validationand hyperparameters for decision trees:\n",
    "* https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "* https://stackoverflow.com/questions/60996995/use-groupkfold-in-nested-cross-validation-using-sklearn\n",
    "* https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
    "* https://scikit-learn.org/stable/modules/tree.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 s, sys: 10.2 s, total: 60 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_full_dataset = XbtDataset(xbt_input_dir, year_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 213 ms, sys: 20.2 ms, total: 233 ms\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_labelled = xbt_full_dataset.filter_obs({'labelled': 'labelled'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.get_ml_dataset(return_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.filter_features(xbt.dataset.TARGET_FEATURES).encode_target(return_data = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer split for ensemble testing\n",
    "As part of this experiment, we will be combining the classifiers on trained each of the splits by using the output of each as avote, and then calculating a pseudo-probability based on the voting. In order to compare the performance of the ensemble voting classifier results with individual classifiers, we need some data that has not been used to train the ensemble. Each individual classifier created excludes a fold of the data for testing by default as part of cross-validation, which is good ML practice. But when all the classifiers are combined, the ensemble has been trained on all the data presented to the cross-validation algorithm, so we need a separate test set for evaluating the metrics of the ensemble classifier. So as for inside cross-validation, we will select some cruises which we hold back and then use at the end for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_unseen_cruise_numbers = xbt_labelled.sample_feature_values(unseen_feature, fraction=ensemble_unseen_fraction, split_feature='year')    \n",
    "ensemble_unseen_cruise_numbers = xbt_labelled.sample_feature_values(unseen_feature, fraction=ensemble_unseen_fraction)    \n",
    "xbt_ens_unseen = xbt_labelled.filter_obs({unseen_feature: ensemble_unseen_cruise_numbers}, mode='include', check_type='in_filter_set')\n",
    "xbt_ens_working = xbt_labelled.filter_obs({unseen_feature: ensemble_unseen_cruise_numbers}, mode='exclude', check_type='in_filter_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.2 s, sys: 16.9 ms, total: 30.3 s\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_ens_working.generate_folds_by_feature('cruise_number', num_unseen_splits, unseen_fold_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruise_numbers = list(xbt_ens_working['cruise_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'US040751'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cruise_numbers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3]),\n",
       " array([3]),\n",
       " array([3]),\n",
       " array([4]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([4]),\n",
       " array([3]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([4]),\n",
       " array([3]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([4]),\n",
       " array([0]),\n",
       " array([0])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xbt_ens_working.filter_obs({'cruise_number': cn1}).xbt_df[unseen_fold_name].unique() for cn1 in cruise_numbers[1000:1020]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labelled = xbt_ens_working.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "y_labelled = xbt_ens_working.filter_features([target_feature]).get_ml_dataset()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cv1 = sklearn.model_selection.GroupKFold(n_splits=num_unseen_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_dt1 = classifier_class(**classifier_opts)\n",
    "clf_gridcv1 = sklearn.model_selection.GridSearchCV(clf_dt1,\n",
    "                                                   param_grid = classifier_param_grid,                                          \n",
    "                                                   scoring=hp_tune_metric,\n",
    "                                                   cv=num_train_splits,\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = sklearn.model_selection.cross_validate(\n",
    "    clf_gridcv1,\n",
    "    X_labelled, y_labelled, \n",
    "    groups=xbt_ens_working[unseen_fold_name], \n",
    "    cv=group_cv1,\n",
    "    return_estimator=True,\n",
    "    return_train_score=True,\n",
    "    scoring=cv_metrics,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate probabilities and evaluate\n",
    "\n",
    "In the outer cross-validation, we trained one classifier for each split. Ultimately we need to choose one set of predictions, which one is best? Instead of picking one, we are creating a voting based ensemble classifier, rather than just retraining on all of the data. To create this, we sue the prediction from each classifier and for each profile, consider the output of a classifier as a vote, and find which instrument type gets the most votes. We can specify this as a pseudo probability by dividing the number of votes for each instrument type by the total number of votes (i.e. the total number of classifiers). The instrument type with maximum probabilty is the classification chosen, but this also gives some idea of uncertainty, as where the classifiers disagree, we know we are less certain about the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_feature_template = 'instrument_res_dt_split{split_num}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res_ens_working = {'id': xbt_ens_working.xbt_df['id']}\n",
    "res_ens_unseen = {'id': xbt_ens_unseen.xbt_df['id']}\n",
    "vote_count_working = numpy.zeros([xbt_ens_working.shape[0], xbt_labelled._feature_encoders[target_feature].classes_.shape[0]],dtype=numpy.float64)\n",
    "vote_count_unseen = numpy.zeros([xbt_ens_unseen.shape[0], xbt_labelled._feature_encoders[target_feature].classes_.shape[0]],dtype=numpy.float64)\n",
    "result_feature_names = []\n",
    "# classifications_df = None\n",
    "for split_num, estimator in enumerate(scores['estimator']):\n",
    "    res_name = result_feature_template.format(split_num=split_num)\n",
    "    result_feature_names += [res_name]\n",
    "    res_ml1_working = estimator.predict(xbt_ens_working.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "    res2_working = xbt_ens_working._feature_encoders[target_feature].inverse_transform(res_ml1_working).reshape(-1,1)\n",
    "    res_ens_working[res_name] = res2_working.reshape(-1)\n",
    "    vote_count_working += xbt_ens_working._target_encoders[target_feature].transform(res2_working)\n",
    "    \n",
    "    res_ml1_unseen = estimator.predict(xbt_ens_unseen.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "    res2_unseen = xbt_ens_unseen._feature_encoders[target_feature].inverse_transform(res_ml1_unseen).reshape(-1,1)\n",
    "    res_ens_unseen[res_name] = res2_unseen.reshape(-1)\n",
    "    vote_count_unseen += xbt_ens_unseen._target_encoders[target_feature].transform(res2_unseen)\n",
    "    \n",
    "df_ens_working = pandas.DataFrame(res_ens_working)\n",
    "df_ens_unseen = pandas.DataFrame(res_ens_unseen)\n",
    "\n",
    "vote_count_working /= float(len(res_ens_working.keys()))    \n",
    "vote_count_unseen /= float(len(res_ens_working.keys()))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prob_feature_name = f'{target_feature}_max_prob'\n",
    "res_working_ensemble = vote_count_working.argmax(axis=1)\n",
    "instr_res_working_ensemble = xbt_labelled._feature_encoders[target_feature].inverse_transform(res_working_ensemble)\n",
    "df_ens_working[max_prob_feature_name] = instr_res_working_ensemble\n",
    "res_unseen_ensemble = vote_count_unseen.argmax(axis=1)\n",
    "instr_res_unseen_ensemble = xbt_labelled._feature_encoders[target_feature].inverse_transform(res_unseen_ensemble)\n",
    "df_ens_unseen[max_prob_feature_name] = instr_res_unseen_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ens_working = pandas.merge(df_ens_working, xbt_ens_working.xbt_df[['id', 'year']])\n",
    "df_ens_unseen = pandas.merge(df_ens_unseen, xbt_ens_unseen.xbt_df[['id', 'year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list_ens = []\n",
    "for year in range(year_range[0],year_range[1]):\n",
    "    y_ens_working = xbt_ens_working.filter_obs({'year': year} ).filter_features([target_feature]).get_ml_dataset()[0]\n",
    "    y_ens_unseen = xbt_ens_unseen.filter_obs({'year': year} ).filter_features([target_feature]).get_ml_dataset()[0]\n",
    "\n",
    "\n",
    "    y_res_working = xbt_ens_working._feature_encoders[target_feature].transform( df_ens_working[df_ens_working.year == year]['instrument_max_prob'])    \n",
    "    y_res_unseen = xbt_ens_unseen._feature_encoders[target_feature].transform( df_ens_unseen[df_ens_unseen.year == year]['instrument_max_prob'])    \n",
    "    cats = list(xbt_ens_working._feature_encoders[target_feature].classes_)\n",
    "    prec_ens_working, recall_ens_working, f1_ens_working, support_ens_working = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_ens_working, y_res_working, average='micro', labels=range(0,len(cats)))\n",
    "    prec_ens_unseen, recall_ens_unseen, f1_ens_unseen, support_ens_unseen = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_ens_unseen, y_res_unseen, average='micro', labels=range(0,len(cats)))\n",
    "\n",
    "    column_template = '{metric}_{data}_{subset}'\n",
    "    metric_dict = {'year': year,\n",
    "                   column_template.format(data='ens_working', metric='precision', subset='all'): prec_ens_working,\n",
    "                   column_template.format(data='ens_working', metric='recall', subset='all'): recall_ens_working,\n",
    "                   column_template.format(data='ens_working', metric='f1', subset='all'): f1_ens_working,\n",
    "                   column_template.format(data='ens_unseen', metric='precision', subset='all'): prec_ens_unseen,\n",
    "                   column_template.format(data='ens_unseen', metric='recall', subset='all'): recall_ens_unseen,\n",
    "                   column_template.format(data='ens_unseen', metric='f1', subset='all'): f1_ens_unseen,\n",
    "                  }\n",
    "\n",
    "    metric_list_ens += [metric_dict]\n",
    "metrics_df_ens = pandas.DataFrame.from_records(metric_list_ens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(xbt_ens_working._feature_encoders[target_feature].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ens_working = xbt_ens_working.filter_features([target_feature]).get_ml_dataset()[0]\n",
    "y_res_working = xbt_ens_working._feature_encoders[target_feature].transform( df_ens_working['instrument_max_prob'])    \n",
    "prec_ens_working, recall_ens_working, f1_ens_working, support_ens_working = sklearn.metrics.precision_recall_fscore_support(\n",
    "    y_ens_working, y_res_working, average='micro', labels=range(0,len(cats)))\n",
    "\n",
    "column_template = '{metric}_{data}_{subset}'\n",
    "scores_ens_working = {\n",
    "    'name': 'ens_working',\n",
    "    'subset': 'train',\n",
    "    'precision_all': prec_ens_working,\n",
    "    'recall_all': recall_ens_working,\n",
    "    'f1_all': f1_ens_working,\n",
    "    'support_all': support_ens_working,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ens_unseen = xbt_ens_unseen.filter_features([target_feature]).get_ml_dataset()[0]\n",
    "y_res_unseen = xbt_ens_unseen._feature_encoders[target_feature].transform( df_ens_unseen['instrument_max_prob'])    \n",
    "prec_ens_unseen, recall_ens_unseen, f1_ens_unseen, support_ens_unseen = sklearn.metrics.precision_recall_fscore_support(\n",
    "    y_ens_unseen, y_res_unseen, average='micro', labels=range(0,len(cats)))\n",
    "scores_ens_unseen = {\n",
    "    'name': 'ens_unseen',\n",
    "    'subset': 'unseen',\n",
    "    'precision_all': prec_ens_unseen,\n",
    "    'recall_all': recall_ens_unseen,\n",
    "    'f1_all': f1_ens_unseen,\n",
    "    'support_all': f1_ens_unseen,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_working = df_ens_working[result_feature_names].apply(lambda row1: len(set(row1.values)), axis='columns')\n",
    "count_working.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unseen = df_ens_unseen[result_feature_names].apply(lambda row1: len(set(row1.values)), axis='columns')\n",
    "count_unseen.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results of cross-validation and hyperparameter tuning\n",
    "In this section we will take a look at the results from the cross validation and hyperparameter tuning of the decision tree to the XBT labelled data. To gain greater understanding, we seprately score the train and test sets for each year of the data and each class of instrument label. This is calculated separately for eeach of the estimators trained for the outer cross-validation (based on cruise numbers). We calculate the recall, precision and f1 metrics both for each class and averaged over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(clf, xbt_ds, data_label, target_feature, input_feature_names, year_range, subset):\n",
    "    metric_list = []\n",
    "    for year in range(year_range[0],year_range[1]):\n",
    "        X_year = xbt_ds.filter_obs({'year': year}, ).filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "        y_year = xbt_ds.filter_obs({'year': year} ).filter_features([target_feature]).get_ml_dataset()[0]\n",
    "\n",
    "        y_res_year = clf.predict(X_year)\n",
    "        cats = list(xbt_ds._feature_encoders[target_feature].classes_)\n",
    "        prec_year, recall_year, f1_year, support_year = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_year, y_res_year, average='micro', labels=range(0,len(cats)))\n",
    "        prec_cat, recall_cat, f1_cat, support_cat = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_year, y_res_year, labels=range(0,len(cats)))\n",
    "\n",
    "        column_template = '{metric}_{data}_{subset}'\n",
    "        metric_dict = {'year': year,\n",
    "                       column_template.format(data=data_label, metric='precision', subset='all'): prec_year,\n",
    "                       column_template.format(data=data_label, metric='recall', subset='all'): recall_year,\n",
    "                       column_template.format(data=data_label, metric='f1', subset='all'): f1_year,\n",
    "                      }\n",
    "\n",
    "        metric_dict.update({column_template.format(data=data_label, metric='precision', subset=cat): val for cat, val in zip(cats, prec_cat)})\n",
    "        metric_dict.update({column_template.format(data=data_label, metric='recall', subset=cat): val for cat, val in zip(cats, recall_cat)})\n",
    "        metric_dict.update({column_template.format(data=data_label, metric='f1', subset=cat): val for cat, val in zip(cats, f1_cat)})\n",
    "        metric_dict.update({column_template.format(data=data_label, metric='support', subset=cat): val for cat, val in zip(cats, support_cat)})\n",
    "        metric_list += [metric_dict]\n",
    "\n",
    "    metrics_df = pandas.DataFrame.from_records(metric_list)\n",
    "    \n",
    "    X_full = xbt_ds.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "    y_full = xbt_ds.filter_features([target_feature]).get_ml_dataset()[0]\n",
    "    y_res_full = clf.predict(X_full)\n",
    "    prec_full, recall_full, f1_full, support_full = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_full, y_res_full, average='micro')\n",
    "    metrics_full = {\n",
    "        'name': data_label,\n",
    "        'subset': subset,\n",
    "        'precision_all': prec_full,\n",
    "        'recall_all': recall_full,\n",
    "        'f1_all': f1_full,\n",
    "        'support_all': support_full,\n",
    "        }    \n",
    "    return metrics_df, metrics_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics_list = {}\n",
    "scores_list = [scores_ens_working, scores_ens_unseen]\n",
    "for split_num, estimator in enumerate(scores['estimator']):\n",
    "    xbt_train = xbt_ens_working.filter_obs({unseen_fold_name: split_num}, mode='exclude')\n",
    "    xbt_test = xbt_ens_working.filter_obs({unseen_fold_name: split_num}, mode='include')\n",
    "    train_metrics, train_scores = generate_metrics(estimator, xbt_train, 'train_{0}'.format(split_num), target_feature, input_feature_names, year_range, 'train' )\n",
    "    test_metrics, test_scores = generate_metrics(estimator, xbt_test, 'test_{0}'.format(split_num), target_feature, input_feature_names, year_range, 'test')\n",
    "    unseen_metrics, unseen_scores = generate_metrics(estimator, xbt_ens_unseen, 'unseen_{0}'.format(split_num), target_feature, input_feature_names, year_range, 'unseen')\n",
    "    scores_list += [train_scores, test_scores, unseen_scores]\n",
    "    metrics_list[split_num] = pandas.merge(pandas.merge(train_metrics, test_metrics, on='year'), unseen_metrics, on='year')\n",
    "scores_df = pandas.DataFrame.from_records(scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[scores_df.subset == 'unseen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[scores_df.subset == 'unseen'].plot.bar(x='name',y='recall_all',figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the recall of the ensemble generally matches the best value of any individual classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_results = matplotlib.pyplot.figure('xbt_results',figsize=(24,40))\n",
    "\n",
    "ax_precision = fig_results.add_subplot(6,3,1, title='precision ensemble')\n",
    "metrics_df_ens.plot.line(x='year',y=['precision_ens_working_all', 'precision_ens_unseen_all'], ax=ax_precision,color=['r','g'])\n",
    "ax_recall = fig_results.add_subplot(6,3,2, title='recall ensemble')\n",
    "metrics_df_ens.plot.line(x='year',y=['recall_ens_working_all', 'recall_ens_unseen_all'],ax=ax_recall,color=['r','g'])\n",
    "ax_f1 = fig_results.add_subplot(6,3,3, title='f1 ensemble')\n",
    "metrics_df_ens.plot.line(x='year',y=['f1_ens_working_all', 'f1_ens_unseen_all'],ax=ax_f1,color=['r','g'])\n",
    "\n",
    "for label1, metrics1  in metrics_list.items():\n",
    "    ax_precision = fig_results.add_subplot(6,3,label1*3 +4, title='precision split {0}'.format(label1))\n",
    "    ax_recall = fig_results.add_subplot(6,3,label1*3 + 4 + 1, title='recall split {0}'.format(label1))\n",
    "    ax_f1 = fig_results.add_subplot(6,3,label1*3 + 4 + 2, title='f1 split {0}'.format(label1))\n",
    "    metrics1.plot.line(ax=ax_precision, x='year', y=[f'precision_train_{label1}_all',f'precision_test_{label1}_all', f'precision_unseen_{label1}_all' ], color=['b', 'r', 'g'], ylim=(0.4,1.0))\n",
    "    metrics1.plot.line(ax=ax_recall, x='year', y=[f'recall_train_{label1}_all',f'recall_test_{label1}_all', f'recall_unseen_{label1}_all'], color=['b', 'r', 'g'], ylim=(0.4,1.0))\n",
    "    metrics1.plot.line(ax=ax_f1, x='year', y=[f'f1_train_{label1}_all',f'f1_test_{label1}_all',f'f1_unseen_{label1}_all'], color=['b', 'r', 'g'], ylim=(0.4,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also of interest is what hyper parameters were chosen by the GridCV hyperparameter tuning process. The following listing shows which parameters result in the best results for each split. We see that limiting the number of nodes resulted in better performance and restricting how nodes could split (min_samples_leaf) also varied based o the split. Varying these parameters results in different classifications so we get some profiles with different class assignments between different classifiers.\n",
    "\n",
    "UPDATE: August 2020 - Currently we are finding a substantial degradation in terms of accuracy measured by recall for the hyperparameter tuning compared to just running the cross-validation. This may just be because there are more splits so the training set is smaller, but it may also be a bug in how this is being done. Further analysis is needed before we can use these results in a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[est1.best_params_ for est1 in scores['estimator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_merge = None\n",
    "for label1, metrics1  in metrics_list.items():\n",
    "    if metrics_df_merge is None:\n",
    "        metrics_df_merge = metrics1\n",
    "    else:\n",
    "        metrics_df_merge = pandas.merge(metrics_df_merge, metrics1, on='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in ensemble metrics\n",
    "metrics_df_merge = pandas.merge(metrics_df_merge, metrics_df_ens, on='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output_path = os.path.join(xbt_output_dir,\n",
    "                                   result_fname_template.format(classifier=classifier_name, \n",
    "                                                                suffix=suffix))\n",
    "metrics_df_merge.to_csv(results_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting the data\n",
    "\n",
    "To filter based on what profiles we can use for predicting, we need some way of checking each profile we create these checkers from the labelled dataset, because the subset of data that was used for training determines what subset is valid for prediction. For example, if a country is not present in the training data, then the prediction function won't be able to handle that profile to predict a probe model and manufacturer. Profiles that are not handled by the trained classifier will get the label \"UNKNOWN\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checker functions check each element of the profile metadata that could be a problem. The checkers are constructed from the labelled data subset.\n",
    "checkers_labelled = {f1: c1 for f1, c1 in xbt_labelled.get_checkers().items() if f1 in input_feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_predictable = xbt_full_dataset.filter_predictable(checkers_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_feature_names = []\n",
    "# classifications_df = None\n",
    "for split_num, estimator in enumerate(scores['estimator']):\n",
    "    res_name = result_feature_template.format(split_num=split_num)\n",
    "    result_feature_names += [res_name]\n",
    "    res_ml1 = estimator.predict(xbt_predictable.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "    res2 = list(xbt_labelled._feature_encoders[target_feature].inverse_transform(res_ml1).reshape(-1))\n",
    "    xbt_predictable.xbt_df[res_name] = res2\n",
    "    # Use the recorded instrument type where it exists (i.e. don't  overwrite with ML prediction)\n",
    "    xbt_predictable.xbt_df.loc[xbt_labelled.xbt_df.index, res_name] = xbt_predictable.xbt_df.loc[xbt_labelled.xbt_df.index, target_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imeta_instrument(row1):\n",
    "    return 'XBT: {t1[0]} ({t1[1]})'.format(t1=imeta_classification(row1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imeta_output_flags = []\n",
    "for res_name in result_feature_names:\n",
    "    flag_name = f'imeta_applied_{res_name}'\n",
    "    imeta_output_flags += [flag_name]\n",
    "    xbt_predictable.xbt_df[flag_name] = 0\n",
    "    xbt_predictable.xbt_df.loc[xbt_predictable.xbt_df[xbt_predictable.xbt_df[res_name].isnull()].index, flag_name] = 1\n",
    "    xbt_predictable.xbt_df[flag_name] = xbt_predictable.xbt_df[flag_name].astype('int8')\n",
    "    xbt_predictable.xbt_df.loc[xbt_predictable.xbt_df[xbt_predictable.xbt_df[res_name].isnull()].index, res_name] = \\\n",
    "        xbt_predictable.xbt_df[xbt_predictable.xbt_df[res_name].isnull()].apply(imeta_instrument, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fv_dict = {rfn1: UNKNOWN_STR for rfn1 in result_feature_names}\n",
    "fv_dict.update({f1: 1 for f1 in imeta_output_flags})\n",
    "xbt_full_dataset.merge_features(xbt_predictable, result_feature_names + imeta_output_flags,\n",
    "                               fill_values = fv_dict,\n",
    "                               feature_encoders={rfn1: xbt_labelled._feature_encoders[target_feature] for rfn1 in result_feature_names},\n",
    "                               target_encoders={rfn1: xbt_labelled._target_encoders[target_feature] for rfn1 in result_feature_names},                                \n",
    "                                output_formatters={rfn1: cat_output_formatter for rfn1 in result_feature_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_unknown_inputs = xbt_full_dataset.filter_obs({'classification_quality_flag': 0})\n",
    "imeta_instrument_fallback = xbt_unknown_inputs.xbt_df.apply(imeta_instrument, axis=1)\n",
    "for res_name in result_feature_names:\n",
    "    xbt_full_dataset.xbt_df.loc[xbt_unknown_inputs.xbt_df.index, res_name] = imeta_instrument_fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is unable to predict a class for some of the profiles. This can be either because the classifier cannot produce a result, or because the country is not present in the labelled dataset. In these cases iMeta is used as a fallback. This plot shows for what proportion of profiles this occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame.from_records([dict(zip(['ML tree','imeta'] ,xbt_full_dataset.xbt_df[imf1].value_counts().values)) for imf1 in imeta_output_flags]).plot(kind='bar',stacked=True,title='Number of profiles falling back on iMeta', figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of interest in our classifier ensemble is how many profiles are labelled differently by different classifiers. Here we count the different labels for each profile. We see  that the majority get only 1 label, but a substantial minority get 2 ior 3 labels, and some get as many as 5 (a different classification from each trained instance of the classifier!). For those profile which have been assigned more that one label, we can assign probabilities based on the number of classifiers \"voting\" for each label. This is not a real probability, but give some idea of the confidence we have in assigned labels. It could also be used as a distribution from which to sample for an esemble temperature dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "count1 = xbt_full_dataset.filter_features(result_feature_names).xbt_df.apply(lambda row1: len(set(row1.values)), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vote_count_full = numpy.zeros([xbt_full_dataset.shape[0], len(xbt_full_dataset._target_encoders[result_feature_names[0]].categories_[0])],dtype=numpy.float64)\n",
    "for res_name in result_feature_names:\n",
    "    vote_count_full += xbt_full_dataset.filter_features([res_name]).encode_target()[0]\n",
    "    \n",
    "vote_count_full /= float(len(result_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_full_ensemble = vote_count_full.argmax(axis=1)\n",
    "instr_res_full_ensemble = xbt_labelled._feature_encoders[target_feature].inverse_transform(res_full_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vote_dict = {f'instrument_probability_{cat1}': vote_count_full[:,ix1] for ix1, cat1 in enumerate(xbt_full_dataset._target_encoders[target_feature].categories_[0])}\n",
    "vote_dict.update({'id': xbt_full_dataset['id'],\n",
    "                  max_prob_feature_name: instr_res_full_ensemble\n",
    "                 })\n",
    "vote_df = pandas.DataFrame(vote_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note about cross validation: one can jkeeping doing forever, but we have limited data to test meaningfully each of our choices\n",
    "what choices have we made?\n",
    "what choices could we test further - future work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.xbt_df = xbt_full_dataset.xbt_df.merge(vote_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_full_dataset.output_data(os.path.join(xbt_output_dir, output_fname_template.format(classifier=classifier_name,\n",
    "                                                                                  suffix=suffix)),\n",
    "                             target_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for ML tree with cross-validation and hyperparameter tuning\n",
    "\n",
    "This notebook demonstrates a pipelines for training classifiers that includes the important steps of cross-validation and hyperparameter tuning. This will be what is ultimately used in training algorithms for producing instrument type classifications as part of the larger ocean temperature dataset production process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experimental-current",
   "language": "python",
   "name": "experimental-current"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
