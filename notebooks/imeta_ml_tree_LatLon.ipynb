{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Decision tree - using Latitue/Longitude\n",
    "\n",
    "This build on the machine learning tree approach in the `imeta_ml_tree.ipynb` notebook, but using a different set of inputs.\n",
    "\n",
    "In this notebook, we use\n",
    "* year\n",
    "* max depth\n",
    "* location, given by latitude & longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repo_dir = pathlib.Path().absolute().parent\n",
    "sys.path = [os.path.join(root_repo_dir)] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xbt.dataset\n",
    "from xbt.dataset import XbtDataset, UNKNOWN_STR, cat_output_formatter, check_value_found\n",
    "from xbt.imeta import imeta_classification, XBT_MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some site specific parameters for the notebook\n",
    "try:\n",
    "    environment = os.environ['XBT_ENV_NAME']\n",
    "except KeyError:\n",
    "    environment = 'pangeo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/',\n",
    "    'pangeo': '/data/misc/xbt-data/',\n",
    "}\n",
    "env_date_ranges = {\n",
    "    'MO_scitools': (1966,2015),\n",
    "    'pangeo': (1966,2015)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some dataset specific parameters\n",
    "root_data_dir = root_data_dirs[environment]\n",
    "year_range = env_date_ranges[environment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'nb_single_decisionTree_latLon'\n",
    "classifier_class = sklearn.tree.DecisionTreeClassifier\n",
    "classifier_opts = {'max_depth': 20,\n",
    "                   'min_samples_leaf': 1,\n",
    "                   'criterion': 'gini'\n",
    "                  }\n",
    "classifier_name = 'decision_tree'\n",
    "suffix='countryLatLon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metric_names = ['f1_weighted','precision_weighted','recall_weighted']\n",
    "input_feature_names = ['max_depth', 'year', 'lat', 'lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_name = 'csv_with_imeta'\n",
    "exp_out_dir_name = 'experiment_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_input_dir = os.path.join(root_data_dir, input_dir_name)\n",
    "xbt_output_dir = os.path.join(root_data_dir, exp_out_dir_name, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output for this experiment if it doesn't exist\n",
    "if not os.path.isdir(xbt_output_dir):\n",
    "    os.makedirs(xbt_output_dir)\n",
    "print(f'outputting to {xbt_output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname_template = 'xbt_output_{classifier}_{suffix}.csv'\n",
    "result_fname_template = 'xbt_metrics_{classifier}_{suffix}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_full_dataset = XbtDataset(xbt_input_dir, year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Data preparation is the same as for the standard imeta ML tree notebook, but with latitude and longitude replacing country code as an input feature. As previously the data is split into unseen, test and train for evaluating performance, and same hyper parameter values are used in decision tree. This time we are only using instrument as a target value, as there not shown to be benefit in training for model and manufacturer separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_labelled = xbt_full_dataset.filter_obs({'labelled': 'labelled'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.get_ml_dataset(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.filter_features(['instrument','model','manufacturer']).encode_target(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unseen_cruise_numbers = xbt_labelled.sample_feature_values('cruise_number', fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_unseen = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='include', check_type='in_filter_set')\n",
    "xbt_working = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='exclude', check_type='in_filter_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_classes = xbt_labelled.xbt_df.apply(imeta_classification, axis=1)\n",
    "imeta_model = imeta_classes.apply(lambda t1: t1[0])\n",
    "imeta_manufacturer = imeta_classes.apply(lambda t1: t1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_instrument = imeta_classes.apply(lambda t1: f'XBT: {t1[0]} ({t1[1]})') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently training and evaulating separately for model and manufacturer. We will also need to train and evaulate together as this is ultimately what is wanted (a combined probe model and manufacturer field).\n",
    "\n",
    "We are using the default 80/20 split in scikit-learn for now. Further work will need to do proper cross validation where several different splits are randomly selected to verify our results are not an artifact of the randomly chosen split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_train_all, xbt_test_all = xbt_working.train_test_split(refresh=True, features=['instrument', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = xbt_train_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_test_all = xbt_test_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_unseen_all = xbt_unseen.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "y_instr_train_all = xbt_train_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_instr_test_all = xbt_test_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_instr_unseen_all = xbt_unseen.filter_features(['instrument']).get_ml_dataset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier\n",
    "\n",
    "We train the Decision tree classifier before and calculate F1, precision and recall for train, test and unseen sets, both for each class and averaged across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all = {}\n",
    "metrics_avg_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_dt_instr1 = classifier_class(**classifier_opts)\n",
    "clf_dt_instr1.fit(X_train_all,y_instr_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all['instrument'] = list(xbt_labelled._feature_encoders['instrument'].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_train_instr_all = clf_dt_instr1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_train_all, y_res_train_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_train': metrics1[0],\n",
    "    'recall_instr_train': metrics1[1],\n",
    "    'f1_instr_train': metrics1[2],\n",
    "    'support_instr_train': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_instr_all = clf_dt_instr1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_test_all, y_res_test_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_test': metrics1[0],\n",
    "    'recall_instr_test': metrics1[1],\n",
    "    'f1_instr_test': metrics1[2],\n",
    "    'support_instr_test': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_unseen_instr_all = clf_dt_instr1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_unseen_all, y_res_unseen_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_unseen': metrics1[0],\n",
    "    'recall_instr_unseen': metrics1[1],\n",
    "    'f1_instr_unseen': metrics1[2],\n",
    "    'support_instr_unseen': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_per_class_instr = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'instr' in k1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg = pandas.DataFrame.from_dict({\n",
    "    'target': ['instrument_train','instrument_test', 'instrument_unseen'],\n",
    "    'precision': [v1 for k1,v1 in metrics_avg_all.items() if 'precision' in k1],\n",
    "    'recall': [v1 for k1,v1 in metrics_avg_all.items() if 'recall' in k1],\n",
    "    'f1': [v1 for k1,v1 in metrics_avg_all.items() if 'f1' in k1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification result plots\n",
    "\n",
    "We see that generalisation (train vs test vs unseen) is worse for using only max depth and year compared to using all three imeta input features, but the degradation in performance is only fairly small, showing that these are the 2 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_results_all_dt = matplotlib.pyplot.figure('xbt_results_all_dt', figsize=(24,8))\n",
    "axis_instr_metrics = fig_results_all_dt.add_subplot(121)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument', y=['recall_instr_train','recall_instr_test','recall_instr_unseen'],ax=axis_instr_metrics)\n",
    "axis_instr_support = fig_results_all_dt.add_subplot(122)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument',y=['support_instr_train', 'support_instr_test', 'support_instr_unseen'], ax=axis_instr_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg.plot.bar(figsize=(18,12), x='target', y='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "The contents of the XBT dataset varies over the time period, so previous papers have looked at classification accuracy (recall) year by year to evaluate how performance varies with different distribution of probe types.\n",
    "\n",
    "To do this we apply the classifier to the train and test data for each year separetly and calculate the metrics year by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_year(xbt_df, year, clf, input_features, target_feature):\n",
    "    data_year1 = xbt_df.filter_obs({'year': year}, )\n",
    "    if data_year1.shape[0] == 0:\n",
    "        return [0.0, 0.0, 0.0, 0]\n",
    "    X_year = data_year1.filter_features(input_features).get_ml_dataset()[0]\n",
    "    y_year = data_year1.filter_features([target_feature]).get_ml_dataset()[0]\n",
    "    y_res_year = clf.predict(X_year)\n",
    "    metric_year = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_year, y_res_year, average='micro')\n",
    "    return metric_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress = ipywidgets.IntProgress(min=env_date_ranges[environment][0],\n",
    "                                           max= env_date_ranges[environment][1],\n",
    "                                          description='Evaluating',\n",
    "                                          bar_style='info')\n",
    "eval_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    results_by_year[year] = {\n",
    "        'metric_train_instr' : score_year(xbt_train_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_test_instr' : score_year(xbt_test_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_unseen_instr' : score_year(xbt_unseen, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "    }\n",
    "    eval_progress.value = year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_by_year = pandas.DataFrame.from_dict({ \n",
    "    'year':  list(results_by_year.keys()),\n",
    "    'recall_train_instr' : [m1['metric_train_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_instr' : [m1['metric_test_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_instr' : [m1['metric_unseen_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr_encoder = xbt_labelled._feature_encoders['instrument']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress.value = env_date_ranges[environment][0]\n",
    "imeta_results = []\n",
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    y_imeta_instr = instr_encoder.transform(pandas.DataFrame(imeta_instrument[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_instr1 = instr_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].instrument))\n",
    "    (im_pr_instr, im_rec_instr, im_f1_instr, im_sup_instr) = sklearn.metrics.precision_recall_fscore_support(xbt_instr1, y_imeta_instr,average='micro')\n",
    "    imeta_results += [{'year': year,\n",
    "                       'imeta_instr_recall': im_rec_instr,\n",
    "                       'imeta_instr_precision': im_pr_instr,\n",
    "                      }]\n",
    "    eval_progress.value = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_res_df = pandas.DataFrame.from_records(imeta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pandas.merge(recall_by_year, imeta_res_df).merge(\n",
    "    pandas.DataFrame.from_dict({\n",
    "        'year': xbt_labelled['year'].value_counts(sort=False).index,\n",
    "        'num_samples': xbt_labelled['year'].value_counts(sort=False).values,\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_model_recall_results = matplotlib.pyplot.figure('xbt_model_recall', figsize=(12,12))\n",
    "ax_instr_recall_results = fig_model_recall_results.add_subplot(111, title='XBT instrument recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_instr','recall_test_instr', 'recall_unseen_instr', 'imeta_instr_recall'], ax=ax_instr_recall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results forper year  using only max depth and year agree with the previous plots in this notebook, showing that the results are slightly worse than using either (country, year, max depth) or (lat,lon,max depth, year), but not much worse, supporting the hypothesis that max depth and year contain the most information for predicting probe type, with country or lat/lon contributing less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['improvement_instr'] = results_df.apply(lambda r1: ((r1['recall_test_instr'] /  r1['imeta_instr_recall'])-1)*100.0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num_samples_per_year = matplotlib.pyplot.figure('fig_num_samples_per_year', figsize=(16,8))\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(121, title='number of samples per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['num_samples'],c='purple' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(122, title='improvement instrument per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_instr'], c='green' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(os.path.join(xbt_output_dir,result_fname_template.format(classifier=classifier_name,\n",
    "                                                                       suffix=suffix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing the data\n",
    "\n",
    "To filter based on what profiles we can use for predicting, we need some way of checking each profile we create these checkers from the labelled dataset, because the subset of data that was used for training determines what subset is valid for prediction. For example, if a country is not present in the training data, then the prediction function won't be able to handle that profile to predict a probe model and manufacturer. Profiles that are not handled by the trained classifier will get the label \"UNKNOWN\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checker functions check each element of the profile metadata that could be a problem. The checkers are constructed from the labelled data subset.\n",
    "checkers_labelled = {f1: c1 for f1, c1 in xbt_labelled.get_checkers().items() if f1 in input_feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_feature_name = 'instrument_res_dt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable = xbt_full_dataset.filter_predictable(checkers_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ml1 = clf_dt_instr1.predict(xbt_predictable.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "res2 = list(xbt_labelled._feature_encoders['instrument'].inverse_transform(res_ml1).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable.xbt_df[result_feature_name] = res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.merge_features(xbt_predictable, [result_feature_name],\n",
    "                               fill_values = {result_feature_name: UNKNOWN_STR},\n",
    "                               feature_encoders={result_feature_name: xbt_labelled._feature_encoders['instrument']},\n",
    "                               target_encoders={result_feature_name: xbt_labelled._target_encoders['instrument']},\n",
    "                               output_formatters={result_feature_name: cat_output_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.output_data(os.path.join(xbt_output_dir, output_fname_template.format(classifier=classifier_name,\n",
    "                                                                                  suffix=suffix)),\n",
    "                             target_features=[result_feature_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we have used only the year of profile and max depth, and have shown that these features are the most important out of the three imeta input features (country, max depth, year). This is important because some data XBT data sources, we don't have country information. This shows that we use a fallback set of classifiers trained only on max depth and year for those cases, and we would get performance that is worse than using all three features, but is better than the results from the intelligent metadata algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experimental-current",
   "language": "python",
   "name": "experimental-current"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
