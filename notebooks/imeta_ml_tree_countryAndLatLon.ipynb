{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A machine learning decision tree approach\n",
    "\n",
    "The iMeta algorithm is essentially a decision tree algorithm, where the variables and threshold for the decisions at each step are manually specified based on human analysis. The simplest way to apply machine learning techniques to the problem would be to use a similar structure to iMeta, which is a decision tree, but use standard ML training techiniques to learn the parameters such as what thresholds to use and how many branches/leaves to have in the tree for the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import functools\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repo_dir = pathlib.Path().absolute().parent\n",
    "sys.path = [os.path.join(root_repo_dir)] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xbt.dataset\n",
    "from xbt.dataset import XbtDataset, UNKNOWN_STR, cat_output_formatter, check_value_found\n",
    "from xbt.imeta import imeta_classification, XBT_MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some site specific parameters for the notebook\n",
    "try:\n",
    "    environment = os.environ['XBT_ENV_NAME']\n",
    "except KeyError:\n",
    "    environment = 'pangeo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/',\n",
    "    'pangeo': '/data/misc/xbt-data/',\n",
    "}\n",
    "env_date_ranges = {\n",
    "    'MO_scitools': (1966,2015),\n",
    "    'pangeo': (1966,2015)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some dataset specific parameters\n",
    "root_data_dir = root_data_dirs[environment]\n",
    "year_range = env_date_ranges[environment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metric_names = ['f1_weighted','precision_weighted','recall_weighted']\n",
    "input_feature_names = ['country','max_depth', 'year', 'lat', 'lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_name = 'csv_with_imeta'\n",
    "exp_out_dir_name = 'experiment_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'nb_single_decisionTree_country'\n",
    "classifier_class = sklearn.tree.DecisionTreeClassifier\n",
    "classifier_name = 'decision_tree'\n",
    "suffix='countryAndLatLon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_opts = {'max_depth': 20,\n",
    "                   'min_samples_leaf': 1,\n",
    "                   'criterion': 'gini'\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_input_dir = os.path.join(root_data_dir, input_dir_name)\n",
    "xbt_output_dir = os.path.join(root_data_dir, exp_out_dir_name, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output for this experiment if it doesn't exist\n",
    "if not os.path.isdir(xbt_output_dir):\n",
    "    os.makedirs(xbt_output_dir)\n",
    "print(f'outputting to {xbt_output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname_template = 'xbt_output_{exp_name}_{subset}.csv'\n",
    "result_fname_template = 'xbt_metrics_{classifier}_{suffix}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_full_dataset = XbtDataset(xbt_input_dir, year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We are only testing on the labelled data, to be able to evluate performance. The XbtDataset class has filtered out some bad data including profiles with maximum depths less that 0.0 or greater than 2000.0. There were also some profiles with bad date entries, which have been excluded for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_labelled = xbt_full_dataset.filter_obs({'labelled': 'labelled'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.get_ml_dataset(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.filter_features(['instrument','model','manufacturer']).encode_target(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unseen_cruise_numbers = xbt_labelled.sample_feature_values('cruise_number', fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_unseen = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='include', check_type='in_filter_set')\n",
    "xbt_working = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='exclude', check_type='in_filter_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_classes = xbt_labelled.xbt_df.apply(imeta_classification, axis=1)\n",
    "imeta_model = imeta_classes.apply(lambda t1: t1[0])\n",
    "imeta_manufacturer = imeta_classes.apply(lambda t1: t1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_instrument = imeta_classes.apply(lambda t1: f'XBT: {t1[0]} ({t1[1]})') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently training and evaulating separately for model and manufacturer. We will also need to train and evaulate together as this is ultimately what is wanted (a combined probe model and manufacturer field).\n",
    "\n",
    "We are using the default 80/20 split in scikit-learn for now. Further work will need to do proper cross validation where several different splits are randomly selected to verify our results are not an artifact of the randomly chosen split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_train_all, xbt_test_all = xbt_working.train_test_split(refresh=True, features=['instrument', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = xbt_train_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_test_all = xbt_test_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_unseen_all = xbt_unseen.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "y_instr_train_all = xbt_train_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_instr_test_all = xbt_test_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_instr_unseen_all = xbt_unseen.filter_features(['instrument']).get_ml_dataset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier\n",
    "\n",
    "We are using the scikit-learn classifier as the closest analogue to the structure of the iMeta algorithm. This tree can have many more nodes and leaves than iMeta though. it is quick to train and evaluate so it is a useful starting point for setting up the ML processing pipelines, as all the scikit-learn classifiers have a common interface. \n",
    "\n",
    "For the model and manufacturer, we train a Decision ree Classifier, then use it to predict values for the train and test sets. We then calculate the accuracy metrics for each for the whole dataset. \n",
    "\n",
    "I am using precision, recall and F1 as fairly standard ML metrics of accuracy. Recall is what has been used in the two previous papers (Palmer et. al, Leahy and Llopis et al) so that is the focus. Support is a useful to see what proportion of the profiles in the dataset belong to each of the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all = {}\n",
    "metrics_avg_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_instr1 = classifier_class(**classifier_opts)\n",
    "clf_dt_instr1.fit(X_train_all,y_instr_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all['instrument'] = list(xbt_labelled._feature_encoders['instrument'].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_train_instr_all = clf_dt_instr1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_train_all, y_res_train_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_train': metrics1[0],\n",
    "    'recall_instr_train': metrics1[1],\n",
    "    'f1_instr_train': metrics1[2],\n",
    "    'support_instr_train': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_instr_all = clf_dt_instr1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_test_all, y_res_test_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_test': metrics1[0],\n",
    "    'recall_instr_test': metrics1[1],\n",
    "    'f1_instr_test': metrics1[2],\n",
    "    'support_instr_test': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_unseen_instr_all = clf_dt_instr1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_unseen_all, y_res_unseen_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_unseen': metrics1[0],\n",
    "    'recall_instr_unseen': metrics1[1],\n",
    "    'f1_instr_unseen': metrics1[2],\n",
    "    'support_instr_unseen': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_per_class_instr = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'instr' in k1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg = pandas.DataFrame.from_dict({\n",
    "    'target': ['instrument_train','instrument_test', 'instrument_unseen'],\n",
    "    'precision': [v1 for k1,v1 in metrics_avg_all.items() if 'precision' in k1],\n",
    "    'recall': [v1 for k1,v1 in metrics_avg_all.items() if 'recall' in k1],\n",
    "    'f1': [v1 for k1,v1 in metrics_avg_all.items() if 'f1' in k1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification result plots\n",
    "\n",
    "The plots below show the results for the whole XBT dataset. We see that the DT classifier performs well on the training data, but does not seem to generalise well. This especially true, as one would expect, for classes with very little support in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_results_all_dt = matplotlib.pyplot.figure('xbt_results_all_dt', figsize=(24,8))\n",
    "axis_instr_metrics = fig_results_all_dt.add_subplot(121)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument', y=['recall_instr_train','recall_instr_test','recall_instr_unseen'],ax=axis_instr_metrics)\n",
    "axis_instr_support = fig_results_all_dt.add_subplot(122)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument',y=['support_instr_train', 'support_instr_test', 'support_instr_unseen'], ax=axis_instr_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg.plot.bar(figsize=(18,12), x='target', y='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "The contents of the XBT dataset varies over the time period, so previous papers have looked at classification accuracy (recall) year by year to evaluate how performance varies with different distribution of probe types.\n",
    "\n",
    "To do this we apply the classifier to the train and test data for each year separetly and calculate the metrics year by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_year(xbt_df, year, clf, input_features, target_feature):\n",
    "    X_year = xbt_df.filter_obs({'year': year}, ).filter_features(input_features).get_ml_dataset()[0]\n",
    "    y_year = xbt_df.filter_obs({'year': year} ).filter_features([target_feature]).get_ml_dataset()[0]\n",
    "    y_res_year = clf.predict(X_year)\n",
    "    metric_year = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_year, y_res_year, average='micro')\n",
    "    return metric_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress = ipywidgets.IntProgress(min=env_date_ranges[environment][0],\n",
    "                                           max= env_date_ranges[environment][1],\n",
    "                                          description='Evaluating',\n",
    "                                          bar_style='info')\n",
    "eval_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    results_by_year[year] = {\n",
    "        'metric_train_instr' : score_year(xbt_train_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_test_instr' : score_year(xbt_test_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_unseen_instr' : score_year(xbt_unseen, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "    }\n",
    "    eval_progress.value = year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_by_year = pandas.DataFrame.from_dict({ \n",
    "    'year':  list(results_by_year.keys()),\n",
    "    'recall_train_instr' : [m1['metric_train_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_instr' : [m1['metric_test_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_instr' : [m1['metric_unseen_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr_encoder = xbt_labelled._feature_encoders['instrument']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress.value = env_date_ranges[environment][0]\n",
    "imeta_results = []\n",
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    y_imeta_instr = instr_encoder.transform(pandas.DataFrame(imeta_instrument[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_instr1 = instr_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].instrument))\n",
    "    (im_pr_instr, im_rec_instr, im_f1_instr, im_sup_instr) = sklearn.metrics.precision_recall_fscore_support(xbt_instr1, y_imeta_instr,average='micro')\n",
    "    imeta_results += [{'year': year,\n",
    "                       'imeta_instr_recall': im_rec_instr,\n",
    "                       'imeta_instr_precision': im_pr_instr,\n",
    "                      }]\n",
    "    eval_progress.value = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_res_df = pandas.DataFrame.from_records(imeta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pandas.merge(recall_by_year, imeta_res_df).merge(\n",
    "    pandas.DataFrame.from_dict({\n",
    "        'year': xbt_labelled['year'].value_counts(sort=False).index,\n",
    "        'num_samples': xbt_labelled['year'].value_counts(sort=False).values,\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_model_recall_results = matplotlib.pyplot.figure('xbt_model_recall', figsize=(12,12))\n",
    "ax_instr_recall_results = fig_model_recall_results.add_subplot(111, title='XBT instrument recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_instr','recall_test_instr', 'recall_unseen_instr', 'imeta_instr_recall'], ax=ax_instr_recall_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['improvement_instr'] = results_df.apply(lambda r1: ((r1['recall_test_instr'] /  r1['imeta_instr_recall'])-1)*100.0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num_samples_per_year = matplotlib.pyplot.figure('fig_num_samples_per_year', figsize=(16,16))\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(121, title='number of samples per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['num_samples'],c='purple' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(122, title='improvement instrument per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_instr'], c='green' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(os.path.join(xbt_output_dir, result_fname_template.format(classifier=classifier_name,\n",
    "                                                                            suffix=suffix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing the data\n",
    "\n",
    "To filter based on what profiles we can use for predicting, we need some way of checking each profile we create these checkers from the labelled dataset, because the subset of data that was used for training determines what subset is valid for prediction. For example, if a country is not present in the training data, then the prediction function won't be able to handle that profile to predict a probe model and manufacturer. Profiles that are not handled by the trained classifier will get the label \"UNKNOWN\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checker functions check each element of the profile metadata that could be a problem. The checkers are constructed from the labelled data subset.\n",
    "checkers_labelled = {f1: c1 for f1, c1 in xbt_labelled.get_checkers().items() if f1 in input_feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_feature_name = 'instrument_res_dt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable = xbt_full_dataset.filter_predictable(checkers_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ml1 = clf_dt_instr1.predict(xbt_predictable.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "res2 = list(xbt_labelled._feature_encoders['instrument'].inverse_transform(res_ml1).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable.xbt_df[result_feature_name] = res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.merge_features(xbt_predictable, [result_feature_name],\n",
    "                               fill_values = {result_feature_name: UNKNOWN_STR},\n",
    "                               feature_encoders={result_feature_name: xbt_labelled._feature_encoders['instrument']},\n",
    "                               target_encoders={result_feature_name: xbt_labelled._target_encoders['instrument']},\n",
    "                               output_formatters={result_feature_name: [cat_output_formatter]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.output_data(out_dir=xbt_output_dir,\n",
    "                             fname_template=output_fname_template,\n",
    "                             exp_name=experiment_name,\n",
    "                             output_split=xbt.common.OUTPUT_SINGLE,\n",
    "                             target_features=[result_feature_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We can see so far that the basic tree approach seems to be outperforming iMeta. Further work needs to be done to calculate these results more rigorously  using cross validation. \n",
    "\n",
    "The next step is to explore more sophisticated tree based approaches, such an ensemble of trees (random forest) and gredient-bossted tress (XGBoost),the current state of the art in tree methods.\n",
    "\n",
    "There are also some issues with the dataset currently being used, so these results may change when the correct version of the data is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_all_labels = list(xbt_labelled._feature_encoders['instrument'].inverse_transform(y_res_test_instr_all).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches = [(i1,i2) for i1,i2 in zip(y_res_test_all_labels, xbt_test_all['instrument']) if i1 != i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experimental-current",
   "language": "python",
   "name": "experimental-current"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
