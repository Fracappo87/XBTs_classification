{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A machine learning decision tree approach\n",
    "\n",
    "The iMeta algorithm is essentially a decision tree algorithm, where the variables and threshold for the decisions at each step are manually specified based on human analysis. The simplest way to apply machine learning techniques to the problem would be to use a similar structure to iMeta, which is a decision tree, but use standard ML training techiniques to learn the parameters such as what thresholds to use and how many branches/leaves to have in the tree for the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repo_dir = pathlib.Path().absolute().parent\n",
    "sys.path = [os.path.join(root_repo_dir)] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes --prefix {sys.prefix} -c anaconda netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(dataexploration.xbt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataexploration.xbt_dataset\n",
    "from dataexploration.xbt_dataset import XbtDataset, UNKNOWN_STR, cat_output_formatter, check_value_found\n",
    "from classification.imeta import imeta_classification, XBT_MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some site specific parameters for the notebook\n",
    "try:\n",
    "    environment = os.environ['XBT_ENV_NAME']\n",
    "except KeyError:\n",
    "    environment = 'azureml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AZURE ML SPECIFIC\n",
    "azure_working_root = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad'\n",
    "xbt_compute_cluster_name = 'xbt-cluster'\n",
    "xbt_vm_size = 'STANDARD_D2_V2'\n",
    "xbt_max_nodes = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/',\n",
    "    'pangeo': '/data/misc/xbt-data/',\n",
    "    'azureml': os.path.join(azure_working_root, 'xbt-data'),\n",
    "}\n",
    "env_date_ranges = {\n",
    "    'MO_scitools': (1966,2015),\n",
    "    'pangeo': (1966,2015),\n",
    "    'azureml': (1966,2015),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some dataset specific parameters\n",
    "root_data_dir = root_data_dirs[environment]\n",
    "year_range = env_date_ranges[environment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'nb_azml_single_decisionTree_country'\n",
    "classifier_class = sklearn.tree.DecisionTreeClassifier\n",
    "classifier_opts = {'max_depth': 20,\n",
    "                   'min_samples_leaf': 1,\n",
    "                   'criterion': 'gini'\n",
    "                  }\n",
    "classifier_name = 'decision_tree'\n",
    "suffix='country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metric_names = ['f1_weighted','precision_weighted','recall_weighted']\n",
    "input_feature_names = ['country','max_depth', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_name = 'csv_with_imeta'\n",
    "exp_out_dir_name = 'experiment_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_json_path = os.path.join(root_repo_dir, 'examples', 'xbt_param_decisionTree_country_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AZURE ML SPECIFIC\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '1fedcbc3-e156-45f5-a034-c89c2fc0ac61'\n",
    "resource_group = 'AWSEarth'\n",
    "workspace_name = 'stephenHaddad_xbt_europeWest'\n",
    "\n",
    "xbt_workspace = Workspace(subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AZURE ML SPECIFIC\n",
    "from azureml.core import Experiment\n",
    "experiment = Experiment(workspace=xbt_workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting directories and retrieve files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_input_dir = os.path.join(root_data_dir, input_dir_name)\n",
    "xbt_output_dir = os.path.join(root_data_dir, exp_out_dir_name, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "azml_dataset = Dataset.get_by_name(xbt_workspace, name='xbt_input_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "tmp_data_dir = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_data_dir = azml_dataset.mount(tmp_data_dir.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_data_dir.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azml_dataset.download(target_path=xbt_input_dir, overwrite=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputting to /mnt/batch/tasks/shared/LS_root/mounts/clusters/xbt-test1/code/Users/stephen.haddad/xbt-data/experiment_outputs/nb_azml_single_decisionTree_country\n"
     ]
    }
   ],
   "source": [
    "# create the output for this experiment if it doesn't exist\n",
    "if not os.path.isdir(xbt_output_dir):\n",
    "    os.makedirs(xbt_output_dir)\n",
    "print(f'outputting to {xbt_output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname_template = 'xbt_output_{classifier}_{suffix}.csv'\n",
    "result_fname_template = 'xbt_metrics_{classifier}_{suffix}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We are only testing on the labelled data, to be able to evaluate performance. The XbtDataset class has filtered out some bad data including profiles with maximum depths less that 0.0 or greater than 2000.0. There were also some profiles with bad date entries, which have been excluded for now.\n",
    "\n",
    "Our training and evaluation will all happen within the labelled dataset, but ultimately we want to apply the trained ML algorithms to the unlabelled data, as this is where labels are needed to reduce the uncertainty in the ocean temperature dataset. So we need to design our experiment with this in mind. What we need to know is how well does our chosen algorithm generalise to unseen data. \n",
    "\n",
    "So within the labelled dataset we split the data into three groups. Firstly, we set a side some fraction of cruises and all profiles that are from those cruises into a group called \"unseen\". These profiles will not be used in the training or initial evaluation. These will represent cruises that have no labelled data. \n",
    "\n",
    "The remainder of the data will then be split into 2 groups, training and test as is standard for a machine learning pipeline. In splitting the data, we will sample a certain fraction of the profiles for train and the rest will be test. But we don't sample from the whole dataset randomly, as this might result in imbalances in representations of years and instrment types. Rather, for each combination of year and instrument label, sample the required fraction from all profiles matching those values. Doing this all possisble value combination of those 2 parameters should results in all years and instrument type being as well represented as possible in both the test and train sets.\n",
    "\n",
    "So the algorithm will be fit to the training set. Then we will evaulate performance on the test set. This gives us an idea of how the algorithm will perform on unlabelled profiles where other profiles from the same cruise are labelled. Then we will evaluate performance of the algorithm on the unseen dataset. This suggests how the algorithm will perform on unlabelled profiles where no profiles from that cruise are labelled. Both of these sorts of profiles have been found to be present in the unlabelled data, so it is important to evaluate the algorithm for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = (1970,1974)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 s, sys: 325 ms, total: 2.2 s\n",
      "Wall time: 9.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_full_dataset = XbtDataset(xbt_input_dir, year_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 101 ms, sys: 0 ns, total: 101 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_labelled = xbt_full_dataset.filter_obs({'labelled': 'labelled'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the data in the pandas dataframe is not suitable for feeding into the machine learning algorithms. We have to appropriately encode the data for training. The dataset class has knowledge of the different types of data represented and how each type should be encoded. This happens through the get_ml_dataset method. Calling this with the return_data flag as False doesn't actually encode the data, but initialises the encoder objects required. These encoders are then used for subsequent encoding operations, so encoding are consistent for the whole experiment and results can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.get_ml_dataset(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.filter_features(['instrument', 'model', 'manufacturer']).encode_target(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 6.64 ms, total: 6.64 ms\n",
      "Wall time: 5.94 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unseen_cruise_numbers = xbt_labelled.sample_feature_values('cruise_number', fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 150 ms, sys: 3.94 ms, total: 154 ms\n",
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_unseen = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='include', check_type='in_filter_set')\n",
    "xbt_working = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='exclude', check_type='in_filter_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 28.1 ms, total: 18.8 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imeta_classes = xbt_labelled.xbt_df.apply(imeta_classification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_model = imeta_classes.apply(lambda t1: t1[0])\n",
    "imeta_manufacturer = imeta_classes.apply(lambda t1: t1[1])\n",
    "imeta_instrument = imeta_classes.apply(lambda t1: f'XBT: {t1[0]} ({t1[1]})') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are initially also interested in investigating classifier performance looking at prove model and manufacturer separately, as well as looking at the combined model and manufacturer info as a single label and using that as the target feature.\n",
    "\n",
    "When splitting the data into test and train sets, we want to ensure that there is an even distribution of years and instrument types in train and test sets. In the train/test split function  used here, we divide the data into years and within the year divide it into instrument types, and then randomly sample from each group. This ensure even represenation of years and instruments. There is also a notebook where the splitting is done randomly with no accounting for class imbalances. The results are very similar, suggesting accuracy is not as sensitive to the precise split as it might seem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 23.6 ms, total: 2.59 s\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_train_all, xbt_test_all = xbt_working.train_test_split(refresh=True, features=['instrument', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = xbt_train_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_test_all = xbt_test_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_unseen_all = xbt_unseen.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "y_model_train_all = xbt_train_all.filter_features(['model']).get_ml_dataset()[0]\n",
    "y_manuf_train_all = xbt_train_all.filter_features(['manufacturer']).get_ml_dataset()[0]\n",
    "y_instr_train_all = xbt_train_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_model_test_all = xbt_test_all.filter_features(['model']).get_ml_dataset()[0]\n",
    "y_manuf_test_all = xbt_test_all.filter_features(['manufacturer']).get_ml_dataset()[0]\n",
    "y_instr_test_all = xbt_test_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_model_unseen_all = xbt_unseen.filter_features(['model']).get_ml_dataset()[0]\n",
    "y_manuf_unseen_all = xbt_unseen.filter_features(['manufacturer']).get_ml_dataset()[0]\n",
    "y_instr_unseen_all = xbt_unseen.filter_features(['instrument']).get_ml_dataset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare/access the compute\n",
    "If we want to use an AzureML clsuter for training, cross-validation, hyperparameter tuning etc. we need to create an object to access (and potentially start up) a suitable compute cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-08-26T13:32:06.248000+00:00', 'errors': None, 'creationTime': '2020-08-26T13:32:02.981314+00:00', 'modifiedTime': '2020-08-26T13:32:18.743834+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D13_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=xbt_workspace, name=xbt_compute_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=xbt_vm_size, \n",
    "                                                           max_nodes=xbt_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(xbt_workspace, xbt_compute_cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier\n",
    "\n",
    "We are using the scikit-learn Decision Tree classifier as the closest analogue to the structure of the iMeta algorithm. This tree can have many more nodes and leaves than iMeta though. it is quick to train and evaluate so it is a useful starting point for setting up the ML processing pipelines, as all the scikit-learn classifiers have a common interface. \n",
    "\n",
    "For each of the target variables (probe model, probe manufacturer, combined probe model/manufacteer label), we train a Decision Tree classifier, then use it to predict values for the train and test sets. We then calculate the accuracy metrics for each for the whole dataset. \n",
    "\n",
    "I am using precision, recall and F1 as fairly standard ML metrics of accuracy. Recall is what has been used in the two previous papers (Palmer et. al, Leahy and Llopis et al) so that is the focus. Support is a useful to see what proportion of the profiles in the dataset belong to each of the different classes.\n",
    "\n",
    "Documentation\n",
    "* https://scikit-learn.org/stable/modules/tree.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all = {}\n",
    "metrics_avg_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all['model'] = list(xbt_labelled._feature_encoders['model'].classes_)\n",
    "metrics_per_class_all['manufacturer'] = list(xbt_labelled._feature_encoders['manufacturer'].classes_)\n",
    "metrics_per_class_all['instrument'] = list(xbt_labelled._feature_encoders['instrument'].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--input-path': xbt_input_dir,\n",
    "    '--output-path': xbt_output_dir,\n",
    "    '--json-experiment': exp_json_path,\n",
    "}\n",
    "conda_packages = ['python=3.8',\n",
    "                  'joblib=0.13.2',\n",
    "                  'pandas=1.0.1',\n",
    "                  'scikit-learn=0.22.1',\n",
    "                  'iris=2.4',\n",
    "                 ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: making this work on AzML\n",
    "* use dataset mount to mount the dataset. this will need to happen on the compute cluster. Don't need to download\n",
    "  * also use mount in the notebooks with local compute\n",
    "* make sure the source code is copied through the source_directory argument\n",
    "* ensure correct conda packages, same for compute instance and cluster\n",
    "  * might need docker longer term\n",
    "* upload experiment file to be used to a run, and then get that file inside the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['joblib', 'scikit-learn']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "estimator = SKLearn(source_directory=str(root_repo_dir), \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='bin/run_single_experiment',\n",
    "#                     pip_packages=['joblib==0.13.2'],\n",
    "                    conda_packages=conda_packages,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_single_run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_single_run.download_file(xbt_single_run.get_file_names()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml-logs/20_image_build_log.txt'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbt_single_run.get_file_names()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config',\n",
       " '.DS_Store',\n",
       " '.ipynb_aml_checkpoints',\n",
       " '.ipynb_checkpoints',\n",
       " '20_image_build_log.txt',\n",
       " 'azml_cluster_imeta_tree.ipynb',\n",
       " 'azml_imeta_tree-OLD.ipynb',\n",
       " 'azml_local_imeta_tree.ipynb',\n",
       " 'batch_script_demo.ipynb',\n",
       " 'classification.py',\n",
       " 'classification_pipeline.py',\n",
       " 'experiment.py',\n",
       " 'first_ml_cross_validate.ipynb',\n",
       " 'first_ml_extend_cruiseSplit.ipynb',\n",
       " 'first_ml_extend_cruiseSplit_latLonCountry.ipynb',\n",
       " 'first_ml_paper.ipynb',\n",
       " 'imeta.py',\n",
       " 'imeta_demo.ipynb',\n",
       " 'imeta_ml_logreg.ipynb',\n",
       " 'imeta_ml_nearest_neighbour.ipynb',\n",
       " 'imeta_ml_randomForest_countryLatLon.ipynb',\n",
       " 'imeta_ml_tree.ipynb',\n",
       " 'imeta_ml_tree_countryAndLatLon.ipynb',\n",
       " 'imeta_ml_tree_cross_validate.ipynb',\n",
       " 'imeta_ml_tree_cvhpt.ipynb',\n",
       " 'imeta_ml_tree_cv_splitYear.ipynb',\n",
       " 'imeta_ml_tree_depthYear.ipynb',\n",
       " 'imeta_ml_tree_LatLon.ipynb',\n",
       " 'imeta_ml_tree_randomSplit.ipynb',\n",
       " 'sklearn_gbtree_demo.ipynb',\n",
       " 'test',\n",
       " 'xbt_classification_results.ipynb',\n",
       " 'xgboost_demo.ipynb',\n",
       " '__init__.py',\n",
       " '__init__.py~',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd216fd977a46549d6ab77482a13997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/nb_azml_single_decisionTree_country/runs/nb_azml_single_decisionTree_country_1598454471_318b10c7?wsid=/subscriptions/1fedcbc3-e156-45f5-a034-c89c2fc0ac61/resourcegroups/AWSEarth/workspaces/stephenHaddad_xbt_europeWest\", \"run_id\": \"nb_azml_single_decisionTree_country_1598454471_318b10c7\", \"run_properties\": {\"run_id\": \"nb_azml_single_decisionTree_country_1598454471_318b10c7\", \"created_utc\": \"2020-08-26T15:07:56.896168Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"23a5ca0b-6b0a-4073-9e21-5630afe345d4\", \"azureml.git.repository_uri\": \"git@github.com:MetOffice/XBTs_classification.git\", \"mlflow.source.git.repoURL\": \"git@github.com:MetOffice/XBTs_classification.git\", \"azureml.git.branch\": \"xbt9_azure_pipelines\", \"mlflow.source.git.branch\": \"xbt9_azure_pipelines\", \"azureml.git.commit\": \"242178541028cab6d6a26285b0e3e3ceb54e47dd\", \"mlflow.source.git.commit\": \"242178541028cab6d6a26285b0e3e3ceb54e47dd\", \"azureml.git.dirty\": \"False\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"resizing\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":0}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-08-26T15:12:07.141366Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/azureml-logs/55_azureml-execution-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt?sv=2019-02-02&sr=b&sig=pyq7M9W%2BcUdhrKrql9g6OEtkMibbr21I%2B%2BQmA7B0L1M%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/azureml-logs/65_job_prep-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt?sv=2019-02-02&sr=b&sig=UOaSXPE%2B3w4D2pGS9%2BTqXebUZh3F58QBRQyChhj4Idw%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=UuZYS3Ej1%2B%2FdmwuVJ0zZ%2FQFWrKid3uHZBZWL3lm1O8g%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"azureml-logs/75_job_post-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/azureml-logs/75_job_post-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt?sv=2019-02-02&sr=b&sig=u50C7RI%2FF8YfM8fcfOn4qvwj7%2B9AOhJ8v5Nb8vRQOnA%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"azureml-logs/process_info.json\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=lGp8uGIMFUrzsRPjMX2EFNxH7hZO%2BIVnFX%2Bg8Sgve3c%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"azureml-logs/process_status.json\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=CF7dKwMTfeyy0lDaLfRqvsZAsVZ0y%2BEHHY5N28gOWC8%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"logs/azureml/134_azureml.log\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/logs/azureml/134_azureml.log?sv=2019-02-02&sr=b&sig=CyCE7RqJ%2BBw%2BA5J2%2FT6ciWAlbRJFGvW4J4CpBDZhqso%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=TsMGhTwlI2YRRgat6lGbb4Ex8m%2BRDsg9XZc3RQHEwmI%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://stephenhaddadx7185391209.blob.core.windows.net/azureml/ExperimentRun/dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=fYy5%2BeGtY3mBf%2BWKiPDPONpl51NMxYa6Dluq0E2vY1o%3D&st=2020-08-26T15%3A02%3A02Z&se=2020-08-26T23%3A12%3A02Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_bf4b9c47d20cdbc803c9dbf2eaff6462ae30e9a95b4c7be08d1b4725f680d274_d.txt\"], [\"logs/azureml/134_azureml.log\"]], \"run_duration\": \"0:04:10\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020-08-26 15:11:53,120|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'EnableMLflowTracking': True, 'snapshotProject': True, 'only_in_process_features': True, 'skip_track_logs_dir': True}, track_folders: None, deny_list: None, directories_to_watch: []\\n2020-08-26 15:11:53,121|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: batchai\\n2020-08-26 15:11:53,132|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2020-08-26 15:11:53,132|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-08-26 15:11:53,561|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7fee7f2432f0> for run source azureml.scriptrun\\n2020-08-26 15:11:53,564|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-08-26 15:11:53,573|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-08-26 15:11:53,574|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-08-26 15:11:53,574|azureml.core.authentication|DEBUG|Time to expire 1814162.425506 seconds\\n2020-08-26 15:11:53,574|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westeurope.experiments.azureml.net.\\n2020-08-26 15:11:53,575|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westeurope.experiments.azureml.net.\\n2020-08-26 15:11:53,575|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westeurope.experiments.azureml.net.\\n2020-08-26 15:11:53,575|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westeurope.experiments.azureml.net.\\n2020-08-26 15:11:53,638|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westeurope.experiments.azureml.net.\\n2020-08-26 15:11:53,638|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westeurope.experiments.azureml.net.\\n2020-08-26 15:11:53,639|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westeurope.experiments.azureml.net.\\n2020-08-26 15:11:53,646|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-08-26 15:11:53,658|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-08-26 15:11:53,665|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-08-26 15:11:53,673|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-08-26 15:11:53,681|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-08-26 15:11:53,681|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-08-26 15:11:53,682|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-08-26 15:11:53,682|msrest.http_logger|DEBUG|Request URL: 'https://westeurope.experiments.azureml.net/history/v1.0/subscriptions/1fedcbc3-e156-45f5-a034-c89c2fc0ac61/resourceGroups/AWSEarth/providers/Microsoft.MachineLearningServices/workspaces/stephenHaddad_xbt_europeWest/experiments/nb_azml_single_decisionTree_country/runs/nb_azml_single_decisionTree_country_1598454471_318b10c7'\\n2020-08-26 15:11:53,683|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-08-26 15:11:53,683|msrest.http_logger|DEBUG|Request headers:\\n2020-08-26 15:11:53,683|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-08-26 15:11:53,683|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-08-26 15:11:53,683|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '4e55a63b-0f2a-4f9f-93a9-e8993d4b495f'\\n2020-08-26 15:11:53,683|msrest.http_logger|DEBUG|    'request-id': '4e55a63b-0f2a-4f9f-93a9-e8993d4b495f'\\n2020-08-26 15:11:53,684|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-stretch-sid) msrest/0.6.17 azureml._restclient/core.1.9.0 azureml-sdk-core/1.9.0'\\n2020-08-26 15:11:53,684|msrest.http_logger|DEBUG|Request body:\\n2020-08-26 15:11:53,684|msrest.http_logger|DEBUG|None\\n2020-08-26 15:11:53,684|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-08-26 15:11:53,684|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-08-26 15:11:53,684|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-08-26 15:11:53,684|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-08-26 15:11:53,803|msrest.http_logger|DEBUG|Response status: 200\\n2020-08-26 15:11:53,804|msrest.http_logger|DEBUG|Response headers:\\n2020-08-26 15:11:53,804|msrest.http_logger|DEBUG|    'Date': 'Wed, 26 Aug 2020 15:11:53 GMT'\\n2020-08-26 15:11:53,804|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-08-26 15:11:53,804|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-08-26 15:11:53,804|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-08-26 15:11:53,804|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-08-26 15:11:53,804|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b'\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|    'x-ms-response-type': 'standard'\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '4e55a63b-0f2a-4f9f-93a9-e8993d4b495f'\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|    'x-request-time': '0.093'\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-08-26 15:11:53,805|msrest.http_logger|DEBUG|Response content:\\n2020-08-26 15:11:53,806|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 1,\\n  \\\"rootRunId\\\": \\\"nb_azml_single_decisionTree_country_1598454471_318b10c7\\\",\\n  \\\"experimentId\\\": \\\"8d304a1c-04e5-49a5-9fcd-5a8e6f2eb7fc\\\",\\n  \\\"createdUtc\\\": \\\"2020-08-26T15:07:56.8961689+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"673965c0-f8fc-435c-ba7b-8b74147bd91b\\\",\\n    \\\"userPuId\\\": \\\"10032000983FE9D5\\\",\\n    \\\"userIdp\\\": \\\"live.com\\\",\\n    \\\"userAltSecId\\\": \\\"1:live.com:000340012189132E\\\",\\n    \\\"userIss\\\": \\\"https://sts.windows.net/14fec308-b428-4380-b914-c1940f3210f1/\\\",\\n    \\\"userTenantId\\\": \\\"14fec308-b428-4380-b914-c1940f3210f1\\\",\\n    \\\"userName\\\": \\\"stephen.haddad\\\"\\n  },\\n  \\\"userId\\\": \\\"673965c0-f8fc-435c-ba7b-8b74147bd91b\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 10,\\n  \\\"runUuid\\\": \\\"4117f046-bac2-4cf5-9f35-d47bebbf87ae\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"4117f046-bac2-4cf5-9f35-d47bebbf87ae\\\",\\n  \\\"runId\\\": \\\"nb_azml_single_decisionTree_country_1598454471_318b10c7\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-08-26T15:10:38.5960503+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.nb_azml_single_decisionTree_country_1598454471_318b10c7\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"23a5ca0b-6b0a-4073-9e21-5630afe345d4\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"git@github.com:MetOffice/XBTs_classification.git\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"git@github.com:MetOffice/XBTs_classification.git\\\",\\n    \\\"azureml.git.branch\\\": \\\"xbt9_azure_pipelines\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"xbt9_azure_pipelines\\\",\\n    \\\"azureml.git.commit\\\": \\\"242178541028cab6d6a26285b0e3e3ceb54e47dd\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"242178541028cab6d6a26285b0e3e3ceb54e47dd\\\",\\n    \\\"azureml.git.dirty\\\": \\\"False\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"run_single_experiment\\\",\\n  \\\"target\\\": \\\"xbt-cluster\\\",\\n  \\\"uniqueChildRunComputeTargets\\\": [],\\n  \\\"tags\\\": {\\n    \\\"_aml_system_ComputeTargetStatus\\\": \\\"{\\\\\\\"AllocationState\\\\\\\":\\\\\\\"resizing\\\\\\\",\\\\\\\"PreparingNodeCount\\\\\\\":0,\\\\\\\"RunningNodeCount\\\\\\\":0,\\\\\\\"CurrentNodeCount\\\\\\\":0}\\\"\\n  },\\n  \\\"inputDatasets\\\": [],\\n  \\\"outputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://westeurope.experiments.azureml.net/execution/v1.0/subscriptions/1fedcbc3-e156-45f5-a034-c89c2fc0ac61/resourceGroups/AWSEarth/providers/Microsoft.MachineLearningServices/workspaces/stephenHaddad_xbt_europeWest/experiments/nb_azml_single_decisionTree_country/runId/nb_azml_single_decisionTree_country_1598454471_318b10c7/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://westeurope.experiments.azureml.net/execution/v1.0/subscriptions/1fedcbc3-e156-45f5-a034-c89c2fc0ac61/resourceGroups/AWSEarth/providers/Microsoft.MachineLearningServices/workspaces/stephenHaddad_xbt_europeWest/experiments/nb_azml_single_decisionTree_country/runId/nb_azml_single_decisionTree_country_1598454471_318b10c7/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-08-26 15:11:53,814|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-08-26 15:11:53,815|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '23a5ca0b-6b0a-4073-9e21-5630afe345d4', 'azureml.git.repository_uri': 'git@github.com:MetOffice/XBTs_classification.git', 'mlflow.source.git.repoURL': 'git@github.com:MetOffice/XBTs_classification.git', 'azureml.git.branch': 'xbt9_azure_pipelines', 'mlflow.source.git.branch': 'xbt9_azure_pipelines', 'azureml.git.commit': '242178541028cab6d6a26285b0e3e3ceb54e47dd', 'mlflow.source.git.commit': '242178541028cab6d6a26285b0e3e3ceb54e47dd', 'azureml.git.dirty': 'False', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-08-26 15:11:53,832|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-08-26 15:11:53,833|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2020-08-26 15:11:53,833|azureml.WorkerPool|DEBUG|[START]\\n2020-08-26 15:11:53,833|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-08-26 15:11:53,833|azureml.RunStatusContext|DEBUG|[START]\\n2020-08-26 15:11:53,833|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-08-26 15:11:53,834|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-08-26 15:11:53,834|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-08-26 15:11:53,834|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/stephenhaddad_xbt_europewest/azureml/nb_azml_single_decisiontree_country_1598454471_318b10c7/mounts/workspaceblobstore/azureml/nb_azml_single_decisionTree_country_1598454471_318b10c7\\n2020-08-26 15:11:53,834|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-08-26 15:11:53,834|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /mnt/batch/tasks/shared/LS_root/jobs/stephenhaddad_xbt_europewest/azureml/nb_azml_single_decisiontree_country_1598454471_318b10c7/mounts/workspaceblobstore/azureml/nb_azml_single_decisionTree_country_1598454471_318b10c7\\n2020-08-26 15:11:54,094|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-08-26 15:11:54,094|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/stephenhaddad_xbt_europewest/azureml/nb_azml_single_decisiontree_country_1598454471_318b10c7/mounts/workspaceblobstore/azureml/nb_azml_single_decisionTree_country_1598454471_318b10c7\\n2020-08-26 15:11:54,094|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /mnt/batch/tasks/shared/LS_root/jobs/stephenhaddad_xbt_europewest/azureml/nb_azml_single_decisiontree_country_1598454471_318b10c7/mounts/workspaceblobstore/azureml/nb_azml_single_decisionTree_country_1598454471_318b10c7 to /mnt/batch/tasks/shared/LS_root/jobs/stephenhaddad_xbt_europewest/azureml/nb_azml_single_decisiontree_country_1598454471_318b10c7/mounts/workspaceblobstore/azureml/nb_azml_single_decisionTree_country_1598454471_318b10c7\\n2020-08-26 15:11:54,094|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /mnt/batch/tasks/shared/LS_root/jobs/stephenhaddad_xbt_europewest/azureml/nb_azml_single_decisiontree_country_1598454471_318b10c7/mounts/workspaceblobstore/azureml/nb_azml_single_decisionTree_country_1598454471_318b10c7\\n2020-08-26 15:11:54,095|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-08-26 15:11:54,095|azureml.WorkingDirectoryCM|ERROR|<class 'ModuleNotFoundError'>: No module named 'classification'\\n<traceback object at 0x7fee7f099688>\\n2020-08-26 15:11:54,095|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-08-26 15:11:54,095|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7|INFO|fail is not setting status for submitted runs.\\n2020-08-26 15:11:54,095|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-08-26 15:11:54,095|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-08-26 15:11:54,096|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-08-26 15:11:54,096|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-08-26 15:11:54,096|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-08-26 15:11:54,096|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2020-08-26 15:11:54,096|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2020-08-26 15:11:54,097|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-08-26 15:11:54,097|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-08-26 15:11:54,097|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-08-26 15:11:54,097|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.PostMetricsBatchV2Daemon|DEBUG|Starting daemon and triggering first instance\\n2020-08-26 15:11:54,098|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-08-26 15:11:54,098|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-08-26 15:11:54,098|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2020-08-26 15:11:54,098|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2020-08-26 15:11:54,098|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|\\n2020-08-26 15:11:54,098|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-08-26 15:11:54,099|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-08-26 15:11:54,099|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[START]\\n2020-08-26 15:11:54,099|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-08-26 15:11:54,100|msrest.http_logger|DEBUG|Request URL: 'https://westeurope.experiments.azureml.net/history/v1.0/subscriptions/1fedcbc3-e156-45f5-a034-c89c2fc0ac61/resourceGroups/AWSEarth/providers/Microsoft.MachineLearningServices/workspaces/stephenHaddad_xbt_europeWest/experiments/nb_azml_single_decisionTree_country/runs/nb_azml_single_decisionTree_country_1598454471_318b10c7/metricsingest/wait'\\n2020-08-26 15:11:54,100|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-08-26 15:11:54,100|msrest.http_logger|DEBUG|Request headers:\\n2020-08-26 15:11:54,100|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-08-26 15:11:54,100|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-08-26 15:11:54,100|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'e20d850a-4df8-45cc-bf8f-319fde2b0d86'\\n2020-08-26 15:11:54,100|msrest.http_logger|DEBUG|    'request-id': 'e20d850a-4df8-45cc-bf8f-319fde2b0d86'\\n2020-08-26 15:11:54,101|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-stretch-sid) msrest/0.6.17 azureml._restclient/core.1.9.0 sdk_run'\\n2020-08-26 15:11:54,101|msrest.http_logger|DEBUG|Request body:\\n2020-08-26 15:11:54,101|msrest.http_logger|DEBUG|None\\n2020-08-26 15:11:54,101|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-08-26 15:11:54,101|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-08-26 15:11:54,101|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-08-26 15:11:54,101|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-08-26 15:11:54,191|msrest.http_logger|DEBUG|Response status: 200\\n2020-08-26 15:11:54,191|msrest.http_logger|DEBUG|Response headers:\\n2020-08-26 15:11:54,191|msrest.http_logger|DEBUG|    'Date': 'Wed, 26 Aug 2020 15:11:54 GMT'\\n2020-08-26 15:11:54,191|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-08-26 15:11:54,192|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-08-26 15:11:54,192|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-08-26 15:11:54,192|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-08-26 15:11:54,192|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b'\\n2020-08-26 15:11:54,192|msrest.http_logger|DEBUG|    'x-ms-response-type': 'standard'\\n2020-08-26 15:11:54,192|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'e20d850a-4df8-45cc-bf8f-319fde2b0d86'\\n2020-08-26 15:11:54,193|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-08-26 15:11:54,193|msrest.http_logger|DEBUG|    'x-request-time': '0.063'\\n2020-08-26 15:11:54,193|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-08-26 15:11:54,193|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-08-26 15:11:54,193|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-08-26 15:11:54,193|msrest.http_logger|DEBUG|Response content:\\n2020-08-26 15:11:54,193|msrest.http_logger|DEBUG|{}\\n2020-08-26 15:11:54,196|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[STOP]\\n2020-08-26 15:11:54,198|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.RunClient.post-async:False|DEBUG|[START]\\n2020-08-26 15:11:54,199|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-08-26 15:11:54,199|msrest.http_logger|DEBUG|Request URL: 'https://westeurope.experiments.azureml.net/history/v1.0/subscriptions/1fedcbc3-e156-45f5-a034-c89c2fc0ac61/resourceGroups/AWSEarth/providers/Microsoft.MachineLearningServices/workspaces/stephenHaddad_xbt_europeWest/experiments/nb_azml_single_decisionTree_country/runs/nb_azml_single_decisionTree_country_1598454471_318b10c7/events'\\n2020-08-26 15:11:54,199|msrest.http_logger|DEBUG|Request method: 'POST'\\n2020-08-26 15:11:54,199|msrest.http_logger|DEBUG|Request headers:\\n2020-08-26 15:11:54,199|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-08-26 15:11:54,200|msrest.http_logger|DEBUG|    'Content-Type': 'application/json-patch+json; charset=utf-8'\\n2020-08-26 15:11:54,200|msrest.http_logger|DEBUG|    'x-ms-caller-name': 'RunHistoryFacade'\\n2020-08-26 15:11:54,200|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '968a852c-5eab-43d5-832c-19e41aeef654'\\n2020-08-26 15:11:54,200|msrest.http_logger|DEBUG|    'request-id': '968a852c-5eab-43d5-832c-19e41aeef654'\\n2020-08-26 15:11:54,200|msrest.http_logger|DEBUG|    'Content-Length': '1422'\\n2020-08-26 15:11:54,201|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-stretch-sid) msrest/0.6.17 azureml._restclient/core.1.9.0 azureml-sdk-core/1.9.0'\\n2020-08-26 15:11:54,201|msrest.http_logger|DEBUG|Request body:\\n2020-08-26 15:11:54,201|msrest.http_logger|DEBUG|{\\\"timestamp\\\": \\\"2020-08-26T15:11:54.198104Z\\\", \\\"name\\\": \\\"Microsoft.MachineLearning.Run.Error\\\", \\\"data\\\": {\\\"RunId\\\": \\\"nb_azml_single_decisionTree_country_1598454471_318b10c7\\\", \\\"ErrorResponse\\\": {\\\"error\\\": {\\\"code\\\": \\\"UserError\\\", \\\"message\\\": \\\"User program failed with ModuleNotFoundError: No module named 'classification'\\\", \\\"detailsUri\\\": \\\"https://aka.ms/azureml-known-errors\\\", \\\"debugInfo\\\": {\\\"type\\\": \\\"ModuleNotFoundError\\\", \\\"message\\\": \\\"No module named 'classification'\\\", \\\"stackTrace\\\": \\\"  File \\\\\\\"/mnt/batch/tasks/shared/LS_root/jobs/stephenhaddad_xbt_europewest/azureml/nb_azml_single_decisiontree_country_1598454471_318b10c7/mounts/workspaceblobstore/azureml/nb_azml_single_decisionTree_country_1598454471_318b10c7/azureml-setup/context_manager_injector.py\\\\\\\", line 166, in execute_with_context\\\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\\\\\"__main__\\\\\\\")\\\\n  File \\\\\\\"/azureml-envs/azureml_b403c202dc2a53c2dfd0192d355f9d04/lib/python3.6/runpy.py\\\\\\\", line 263, in run_path\\\\n    pkg_name=pkg_name, script_name=fname)\\\\n  File \\\\\\\"/azureml-envs/azureml_b403c202dc2a53c2dfd0192d355f9d04/lib/python3.6/runpy.py\\\\\\\", line 96, in _run_module_code\\\\n    mod_name, mod_spec, pkg_name, script_name)\\\\n  File \\\\\\\"/azureml-envs/azureml_b403c202dc2a53c2dfd0192d355f9d04/lib/python3.6/runpy.py\\\\\\\", line 85, in _run_code\\\\n    exec(code, run_globals)\\\\n  File \\\\\\\"run_single_experiment\\\\\\\", line 13, in <module>\\\\n    from classification import classification\\\\n\\\"}}}}}\\n2020-08-26 15:11:54,201|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-08-26 15:11:54,201|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-08-26 15:11:54,202|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-08-26 15:11:54,202|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-08-26 15:11:54,293|msrest.http_logger|DEBUG|Response status: 200\\n2020-08-26 15:11:54,293|msrest.http_logger|DEBUG|Response headers:\\n2020-08-26 15:11:54,293|msrest.http_logger|DEBUG|    'Date': 'Wed, 26 Aug 2020 15:11:54 GMT'\\n2020-08-26 15:11:54,294|msrest.http_logger|DEBUG|    'Content-Length': '0'\\n2020-08-26 15:11:54,294|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-08-26 15:11:54,294|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b'\\n2020-08-26 15:11:54,294|msrest.http_logger|DEBUG|    'x-ms-response-type': 'standard'\\n2020-08-26 15:11:54,294|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '968a852c-5eab-43d5-832c-19e41aeef654'\\n2020-08-26 15:11:54,295|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-08-26 15:11:54,295|msrest.http_logger|DEBUG|    'x-request-time': '0.067'\\n2020-08-26 15:11:54,295|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-08-26 15:11:54,295|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-08-26 15:11:54,295|msrest.http_logger|DEBUG|Response content:\\n2020-08-26 15:11:54,295|msrest.http_logger|DEBUG|\\n2020-08-26 15:11:54,298|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.RunClient.post-async:False|DEBUG|[STOP]\\n2020-08-26 15:11:54,298|azureml.RunStatusContext|ERROR|<class 'ModuleNotFoundError'>: No module named 'classification'\\n<traceback object at 0x7fee7f099688>\\n2020-08-26 15:11:54,298|azureml.RunStatusContext|DEBUG|[STOP]\\n2020-08-26 15:11:54,299|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-08-26 15:11:54,299|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-08-26 15:11:54,299|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-08-26 15:11:54,300|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-08-26 15:11:54,300|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-08-26 15:11:54,300|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-08-26 15:11:54,300|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-08-26 15:11:54,300|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-08-26 15:11:54,300|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-08-26 15:11:54,301|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|\\n2020-08-26 15:11:54,303|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-08-26 15:11:54,303|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-08-26 15:11:54,303|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[START]\\n2020-08-26 15:11:54,304|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-08-26 15:11:54,304|msrest.http_logger|DEBUG|Request URL: 'https://westeurope.experiments.azureml.net/history/v1.0/subscriptions/1fedcbc3-e156-45f5-a034-c89c2fc0ac61/resourceGroups/AWSEarth/providers/Microsoft.MachineLearningServices/workspaces/stephenHaddad_xbt_europeWest/experiments/nb_azml_single_decisionTree_country/runs/nb_azml_single_decisionTree_country_1598454471_318b10c7/metricsingest/wait'\\n2020-08-26 15:11:54,304|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-08-26 15:11:54,304|msrest.http_logger|DEBUG|Request headers:\\n2020-08-26 15:11:54,305|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-08-26 15:11:54,305|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-08-26 15:11:54,305|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'c3c4afa9-54f7-4c66-82b6-1a06cb9d6ad2'\\n2020-08-26 15:11:54,305|msrest.http_logger|DEBUG|    'request-id': 'c3c4afa9-54f7-4c66-82b6-1a06cb9d6ad2'\\n2020-08-26 15:11:54,305|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-stretch-sid) msrest/0.6.17 azureml._restclient/core.1.9.0 sdk_run'\\n2020-08-26 15:11:54,305|msrest.http_logger|DEBUG|Request body:\\n2020-08-26 15:11:54,306|msrest.http_logger|DEBUG|None\\n2020-08-26 15:11:54,306|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-08-26 15:11:54,306|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-08-26 15:11:54,306|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-08-26 15:11:54,306|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-08-26 15:11:54,399|msrest.http_logger|DEBUG|Response status: 200\\n2020-08-26 15:11:54,400|msrest.http_logger|DEBUG|Response headers:\\n2020-08-26 15:11:54,400|msrest.http_logger|DEBUG|    'Date': 'Wed, 26 Aug 2020 15:11:54 GMT'\\n2020-08-26 15:11:54,400|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-08-26 15:11:54,400|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-08-26 15:11:54,400|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-08-26 15:11:54,400|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-08-26 15:11:54,401|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b'\\n2020-08-26 15:11:54,401|msrest.http_logger|DEBUG|    'x-ms-response-type': 'standard'\\n2020-08-26 15:11:54,401|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'c3c4afa9-54f7-4c66-82b6-1a06cb9d6ad2'\\n2020-08-26 15:11:54,401|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-08-26 15:11:54,401|msrest.http_logger|DEBUG|    'x-request-time': '0.059'\\n2020-08-26 15:11:54,401|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-08-26 15:11:54,401|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-08-26 15:11:54,402|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-08-26 15:11:54,403|msrest.http_logger|DEBUG|Response content:\\n2020-08-26 15:11:54,403|msrest.http_logger|DEBUG|{}\\n2020-08-26 15:11:54,405|azureml._SubmittedRun#nb_azml_single_decisionTree_country_1598454471_318b10c7.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[STOP]\\n2020-08-26 15:11:54,406|azureml.SendRunKillSignal|ERROR|<class 'ModuleNotFoundError'>: No module named 'classification'\\n<traceback object at 0x7fee7f099688>\\n2020-08-26 15:11:54,406|azureml.SendRunKillSignal|DEBUG|[STOP]\\n2020-08-26 15:11:54,406|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[START]\\n2020-08-26 15:11:54,406|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[STOP]\\n2020-08-26 15:11:54,406|azureml.WorkerPool|ERROR|<class 'ModuleNotFoundError'>: No module named 'classification'\\n<traceback object at 0x7fee7f099688>\\n2020-08-26 15:11:54,407|azureml.WorkerPool|DEBUG|[STOP]\\n\\nError occurred: User program failed with ModuleNotFoundError: No module named 'classification'\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": true, \"log_level\": \"INFO\", \"sdk_version\": \"1.6.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(xbt_single_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_train_model_all = clf_dt_model1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_model_train_all, y_res_train_model_all,  labels=list(range(0,len(metrics_per_class_all['model']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_model_train': metrics1[0],\n",
    "    'recall_model_train': metrics1[1],\n",
    "    'f1_model_train': metrics1[2],\n",
    "    'support_model_train': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_model_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_model_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_model_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_model = clf_dt_model1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_model_test_all, y_res_test_model,  labels=list(range(0,len(metrics_per_class_all['model']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_model_test': metrics1[0],\n",
    "    'recall_model_test': metrics1[1],\n",
    "    'f1_model_test': metrics1[2],\n",
    "    'support_model_test': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_model_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_model_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_model_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_unseen_model = clf_dt_model1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_model_unseen_all, y_res_unseen_model, labels=list(range(0,len(metrics_per_class_all['model']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_model_unseen': metrics1[0],\n",
    "    'recall_model_unseen': metrics1[1],\n",
    "    'f1_model_unseen': metrics1[2],\n",
    "    'support_model_unseen': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_model_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_model_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_model_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_manuf1 = classifier_class(**classifier_opts)\n",
    "clf_dt_manuf1.fit(X_train_all,y_manuf_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_train_manuf_all = clf_dt_manuf1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_manuf_train_all, y_res_train_manuf_all, labels=list(range(0,len(metrics_per_class_all['manufacturer']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_manuf_train': metrics1[0],\n",
    "    'recall_manuf_train': metrics1[1],\n",
    "    'f1_manuf_train': metrics1[2],\n",
    "    'support_manuf_train': metrics1[3],\n",
    "})\n",
    "\n",
    "metrics_avg_all.update({\n",
    "    'precision_manuf_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_manuf_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_manuf_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_manuf_all = clf_dt_manuf1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_manuf_test_all, y_res_test_manuf_all, labels=list(range(0,len(metrics_per_class_all['manufacturer']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_manuf_test': metrics1[0],\n",
    "    'recall_manuf_test': metrics1[1],\n",
    "    'f1_manuf_test': metrics1[2],\n",
    "    'support_manuf_test': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_manuf_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_manuf_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_manuf_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_unseen_manuf_all = clf_dt_manuf1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_manuf_unseen_all, y_res_unseen_manuf_all, labels=list(range(0,len(metrics_per_class_all['manufacturer']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_manuf_unseen': metrics1[0],\n",
    "    'recall_manuf_unseen': metrics1[1],\n",
    "    'f1_manuf_unseen': metrics1[2],\n",
    "    'support_manuf_unseen': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_manuf_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_manuf_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_manuf_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_instr1 = classifier_class(**classifier_opts)\n",
    "clf_dt_instr1.fit(X_train_all,y_instr_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_res_train_instr_all = clf_dt_instr1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_train_all, y_res_train_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_train': metrics1[0],\n",
    "    'recall_instr_train': metrics1[1],\n",
    "    'f1_instr_train': metrics1[2],\n",
    "    'support_instr_train': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_res_test_instr_all = clf_dt_instr1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_test_all, y_res_test_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_test': metrics1[0],\n",
    "    'recall_instr_test': metrics1[1],\n",
    "    'f1_instr_test': metrics1[2],\n",
    "    'support_instr_test': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_res_unseen_instr_all = clf_dt_instr1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_unseen_all, y_res_unseen_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))) )\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_unseen': metrics1[0],\n",
    "    'recall_instr_unseen': metrics1[1],\n",
    "    'f1_instr_unseen': metrics1[2],\n",
    "    'support_instr_unseen': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_per_class_model = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'model' in k1})\n",
    "df_metrics_per_class_manuf = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'manuf' in k1})\n",
    "df_metrics_per_class_instr = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'instr' in k1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg = pandas.DataFrame.from_dict({\n",
    "    'target': ['model_train','model_test', 'model_unseen', 'manufacturer_train','manufacturer_test', 'manufacturer_unseen','instrument_train','instrument_test', 'instrument_unseen'],\n",
    "    'precision': [v1 for k1,v1 in metrics_avg_all.items() if 'precision' in k1],\n",
    "    'recall': [v1 for k1,v1 in metrics_avg_all.items() if 'recall' in k1],\n",
    "    'f1': [v1 for k1,v1 in metrics_avg_all.items() if 'f1' in k1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification result plots\n",
    "\n",
    "The plots below show the results for the whole XBT dataset. We see that the DT classifier performs almost perfectly on the training data, but performance degrades for the test data, and further still for the profiles from unseen cruises. So it does not seem to generalise that well, but results are still better than other algorithms where the train and test results are closer together. Performance is poor for classes with very little support in the training dataset, as one might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_per_class_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_results_all_dt = matplotlib.pyplot.figure('xbt_results_all_dt', figsize=(24,24))\n",
    "axis_model_metrics = fig_results_all_dt.add_subplot(321)\n",
    "_ = df_metrics_per_class_model.plot.bar(x='model',y=['recall_model_train', 'recall_model_test', 'recall_model_unseen'], ax=axis_model_metrics)\n",
    "axis_model_support = fig_results_all_dt.add_subplot(322)\n",
    "_ = df_metrics_per_class_model.plot.bar(x='model',y=['support_model_train', 'support_model_test', 'support_model_unseen'], ax=axis_model_support)\n",
    "axis_manuf_metrics = fig_results_all_dt.add_subplot(323)\n",
    "_ = df_metrics_per_class_manuf.plot.bar(x='manufacturer', y=['recall_manuf_train','recall_manuf_test','recall_manuf_unseen'],ax=axis_manuf_metrics)\n",
    "axis_manuf_support = fig_results_all_dt.add_subplot(324)\n",
    "_ = df_metrics_per_class_manuf.plot.bar(x='manufacturer',y=['support_manuf_train', 'support_manuf_test', 'support_manuf_unseen'], ax=axis_manuf_support)\n",
    "axis_instr_metrics = fig_results_all_dt.add_subplot(325)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument', y=['recall_instr_train','recall_instr_test','recall_instr_unseen'],ax=axis_instr_metrics)\n",
    "axis_instr_support = fig_results_all_dt.add_subplot(326)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument',y=['support_instr_train', 'support_instr_test', 'support_instr_unseen'], ax=axis_instr_support)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of performance per class show that results generalise best where there are a substantial number of profiles of that probe type in the training and test sets. Further work may be needed to consider how to boost the performance of underepresented classes. Although these may be only a small number of profiles, we should also consider whether the measurements they contribute are in under represented areas and thus may have a disproportionately alrge impact on the final teprature values in some parts of the world in some years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg.plot.bar(figsize=(18,12), x='target', y='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "The contents of the XBT dataset varies over the time period, so previous papers have looked at classification accuracy (recall) year by year to evaluate how performance varies with different distribution of probe types.\n",
    "\n",
    "To do this we apply the classifier to the train, test  and unseen profile groups for each year separately. For each year we calculate the target metrics for each taret variable and plot the results. We compare the results to those achieved by the iMeta algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_year(xbt_df, year, clf, input_features, target_feature):\n",
    "    X_year = xbt_df.filter_obs({'year': year}, ).filter_features(input_features).get_ml_dataset()[0]\n",
    "    y_year = xbt_df.filter_obs({'year': year} ).filter_features([target_feature]).get_ml_dataset()[0]\n",
    "    y_res_year = clf.predict(X_year)\n",
    "    metric_year = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_year, y_res_year, average='micro')\n",
    "    return metric_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress = ipywidgets.IntProgress(min=env_date_ranges[environment][0],\n",
    "                                           max= env_date_ranges[environment][1],\n",
    "                                          description='Evaluating',\n",
    "                                          bar_style='info')\n",
    "eval_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    results_by_year[year] = {\n",
    "        'metric_train_model' : score_year(xbt_train_all, year, clf_dt_model1, input_feature_names, 'model'),\n",
    "        'metric_test_model' : score_year(xbt_test_all, year, clf_dt_model1, input_feature_names, 'model'),\n",
    "        'metric_unseen_model' : score_year(xbt_unseen, year, clf_dt_model1, input_feature_names, 'model'),\n",
    "        'metric_train_manuf' : score_year(xbt_train_all, year, clf_dt_manuf1, input_feature_names, 'manufacturer'),\n",
    "        'metric_test_manuf' : score_year(xbt_test_all, year, clf_dt_manuf1, input_feature_names, 'manufacturer'),\n",
    "        'metric_unseen_manuf' : score_year(xbt_unseen, year, clf_dt_manuf1, input_feature_names, 'manufacturer'),\n",
    "        'metric_train_instr' : score_year(xbt_train_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_test_instr' : score_year(xbt_test_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_unseen_instr' : score_year(xbt_unseen, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "    }\n",
    "    eval_progress.value = year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_by_year = pandas.DataFrame.from_dict({ \n",
    "    'year':  list(results_by_year.keys()),\n",
    "    'recall_train_model' : [m1['metric_train_model'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_model' : [m1['metric_test_model'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_model' : [m1['metric_unseen_model'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_train_manuf' : [m1['metric_train_manuf'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_manuf' : [m1['metric_test_manuf'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_manuf' : [m1['metric_unseen_manuf'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_train_instr' : [m1['metric_train_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_instr' : [m1['metric_test_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_instr' : [m1['metric_unseen_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = xbt_labelled._feature_encoders['model']\n",
    "manuf_encoder = xbt_labelled._feature_encoders['manufacturer']\n",
    "instr_encoder = xbt_labelled._feature_encoders['instrument']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_progress.value = env_date_ranges[environment][0]\n",
    "imeta_results = []\n",
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    y_imeta_model = model_encoder.transform(pandas.DataFrame(imeta_model[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_model1 = model_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].model))\n",
    "\n",
    "    y_imeta_manuf = manuf_encoder.transform(pandas.DataFrame(imeta_manufacturer[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_manufacturer1 = manuf_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].manufacturer))\n",
    "\n",
    "    y_imeta_instr = instr_encoder.transform(pandas.DataFrame(imeta_instrument[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_instr1 = instr_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].instrument))\n",
    "    \n",
    "    \n",
    "    (im_pr_model, im_rec_model, im_f1_model, im_sup_model) = sklearn.metrics.precision_recall_fscore_support(xbt_model1, y_imeta_model,average='micro')\n",
    "    (im_pr_manuf, im_rec_manuf, im_f1_manuf, im_sup_manuf) = sklearn.metrics.precision_recall_fscore_support(xbt_manufacturer1, y_imeta_manuf,average='micro')\n",
    "    (im_pr_instr, im_rec_instr, im_f1_instr, im_sup_instr) = sklearn.metrics.precision_recall_fscore_support(xbt_instr1, y_imeta_instr,average='micro')\n",
    "\n",
    "    imeta_results += [{'year': year,\n",
    "                       'imeta_model_recall': im_rec_model,\n",
    "                       'imeta_model_precision': im_pr_model,\n",
    "                       'imeta_manuf_recall': im_rec_manuf,\n",
    "                       'imeta_manuf_precision': im_pr_manuf,\n",
    "                       'imeta_instr_recall': im_rec_instr,\n",
    "                       'imeta_instr_precision': im_pr_instr,\n",
    "                      }]\n",
    "    eval_progress.value = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_res_df = pandas.DataFrame.from_records(imeta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pandas.merge(recall_by_year, imeta_res_df).merge(\n",
    "    pandas.DataFrame.from_dict({\n",
    "        'year': xbt_labelled['year'].value_counts(sort=False).index,\n",
    "        'num_samples': xbt_labelled['year'].value_counts(sort=False).values,\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_model_recall_results = matplotlib.pyplot.figure('xbt_model_recall', figsize=(18,6))\n",
    "ax_model_recall_results = fig_model_recall_results.add_subplot(131, title='XBT model recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_model','recall_test_model', 'recall_unseen_model', 'imeta_model_recall'], ax=ax_model_recall_results)\n",
    "ax_manuf_recall_results = fig_model_recall_results.add_subplot(132, title='XBT manufacturer recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_manuf','recall_test_manuf', 'recall_unseen_manuf', 'imeta_manuf_recall'], ax=ax_manuf_recall_results)\n",
    "ax_instr_recall_results = fig_model_recall_results.add_subplot(133, title='XBT instrument recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_instr','recall_test_instr', 'recall_unseen_instr', 'imeta_instr_recall'], ax=ax_instr_recall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for both the test and unseen profiles, the decision tree performs at least as well as the iMeta algorithm, and substantially better for some years, particularly the 1985-1995 era when there are a lot of profiles and a lot of them are missing probe type meta data. We can see how the algorithm genalises better in some years that others. The performance on the unseen profiles is slightly worse than the test profiles, but roughly comparable except in a few years where performance is substantially worse. This merits further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['improvement_model'] = results_df.apply(lambda r1: ((r1['recall_test_model'] /  r1['imeta_model_recall'])-1)*100.0 , axis=1)\n",
    "results_df['improvement_manuf'] = results_df.apply(lambda r1: ((r1['recall_test_manuf'] /  r1['imeta_manuf_recall'])-1)*100.0 , axis=1)\n",
    "results_df['improvement_instr'] = results_df.apply(lambda r1: ((r1['recall_test_instr'] /  r1['imeta_instr_recall'])-1)*100.0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num_samples_per_year = matplotlib.pyplot.figure('fig_num_samples_per_year', figsize=(16,16))\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(221, title='number of samples per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['num_samples'],c='purple' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(222, title='improvement model per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_model'], c='green' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(223, title='improvement manufacturer per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_manuf'], c='green' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(224, title='improvement instrument per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_instr'], c='green' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see substantial improvements in recall some years, particularly the early 1990s. This is a postive outcome as this where the high percent of unlabelled data is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(os.path.join(xbt_output_dir, result_fname_template.format(classifier=classifier_name,\n",
    "                                                                            suffix=suffix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing the data\n",
    "\n",
    "The output format of the probe classifications is both as a string label, as as a one hot encoding (OHE). OHE econdes the results as as nxm array with each of n rows representing a profile and each of m columns a profile type, with zeroes in most entries except a 1 in the column of the probe type for that profile. This is used as it can be genealised to providing probabilities, where instead of just ones and zeroes, each column represents the probability of the profile being from a particular probe type, with values between 0 and 1 that sum to 1 for each row. This will be explored further in subsequent notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checker functions check each element of the profile metadata that could be a problem. The checkers are constructed from the labelled data subset.\n",
    "checkers_labelled = {f1: c1 for f1, c1 in xbt_labelled.get_checkers().items() if f1 in input_feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_feature_name = 'instrument_res_dt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter based on what profiles we can use for predicting, we need some way of checking each profile we create these checkers from the labelled dataset, because the subset of data that was used for training determines what subset is valid for prediction. For example, if a country is not present in the training data, then the prediction function won't be able to handle that profile to predict a probe model and manufacturer. Profiles that are not handled by the trained classifier will get the label \"UNKNOWN\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_predictable = xbt_full_dataset.filter_predictable(checkers_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ml1 = clf_dt_instr1.predict(xbt_predictable.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "res2 = list(xbt_labelled._feature_encoders['instrument'].inverse_transform(res_ml1).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable.xbt_df[result_feature_name] = res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.merge_features(xbt_predictable, [result_feature_name],\n",
    "                               fill_values = {result_feature_name: UNKNOWN_STR},\n",
    "                               feature_encoders={result_feature_name: xbt_labelled._feature_encoders['instrument']},\n",
    "                                target_encoders={result_feature_name: xbt_labelled._target_encoders['instrument']},\n",
    "                               output_formatters={result_feature_name: cat_output_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_full_dataset.output_data(os.path.join(xbt_output_dir, output_fname_template.format(classifier=classifier_name,\n",
    "                                                                                  suffix=suffix)),\n",
    "                             target_features=[result_feature_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We can see so far that the basic tree xbt_output_dirch seems to be outperforming iMeta. Further work needs to be done to calculate these results more rigorously  using cross validation. \n",
    "\n",
    "The next step is to explore more sophisticated tree based approaches, such an ensemble of trees (random forest) and gredient-bossted tress (XGBoost),the current state of the art in tree methods. We also need to consider other classification appraches such as neural networks, which have already been applied to this dataset in a previous paper, or a nearest neighbour approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
