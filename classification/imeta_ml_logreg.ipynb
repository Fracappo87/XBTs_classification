{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Decision tree - replacing country with Lat/Lon\n",
    "\n",
    "The iMeta algorithm is essentially a decision tree algorithm, where the variables and threshold for the decisions at each step are manually specified based on human analysis. We have shown that good results can be achieved with a standard ML tree compared to the manually constructed iMeta tree algorithm, using the same three input features: country, maximum depth and year. \n",
    "\n",
    "We woukd also like to know whether other features might give a better result. There are also  some issues relating to country where some countries have no labelled profiles, which means the algorithm won't know what to do with that value for the country category. The expectation is the probe type is determined by country of origin, which means the country responsible for deploying the probe rather than the nearest country to where the probe was deployed, since different probes are sold in different markets. This notebook will look at whether there is a relationship between Latitude/Longitude and the probe type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repo_dir = Path().absolute().parent\n",
    "sys.path = [os.path.join(root_repo_dir,'dataexploration'),\n",
    "            os.path.join(root_repo_dir,'classification'),\n",
    "            os.path.join(root_repo_dir,'preprocessing'),\n",
    "            os.path.join(root_repo_dir,'metrics'),\n",
    "           ] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbt_dataset import XbtDataset, UNKNOWN_STR, cat_output_formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imeta import imeta_classification, XBT_MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some site specific parameters for the notebook\n",
    "try:\n",
    "    environment = os.environ['XBT_ENV_NAME']\n",
    "except KeyError:\n",
    "    environment = 'pangeo'\n",
    "env_data_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/csv_with_imeta',\n",
    "    'pangeo': '/data/misc/xbt-data/csv_with_imeta/',\n",
    "}\n",
    "env_date_ranges = {\n",
    "    'MO_scitools': (1966,2015),\n",
    "    'pangeo': (1966,2015)\n",
    "}\n",
    "output_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/csv_output',\n",
    "    'pangeo': '/data/misc/xbt-data/csv_output/',\n",
    "\n",
    "}\n",
    "result_dirs = {\n",
    "    'MO_scitools': '/data/users/shaddad/xbt-data/results',\n",
    "    'pangeo': '/data/misc/xbt-data/results/',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = output_dirs[environment]\n",
    "output_fname_template = 'xbt_output_{classifier}_{suffix}.csv'\n",
    "result_dir = result_dirs[environment]\n",
    "result_fname_template = 'xbt_metrics_{classifier}_{suffix}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metric_names = ['f1_weighted','precision_weighted','recall_weighted']\n",
    "input_feature_names = ['max_depth', 'year', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_class = sklearn.linear_model.LogisticRegression\n",
    "classifier_opts = {}\n",
    "classifier_name = 'logreg'\n",
    "suffix = 'country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 10.1 s, total: 2min 13s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_full_dataset = XbtDataset(env_data_dirs[environment], env_date_ranges[environment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Data preparation is the same as for the standard imeta ML tree notebook, but with latitude and longitude replacing country code as an input feature. As previously the data is split into unseen, test and train for evaluating performance, and same hyper parameter values are used in decision tree. This time we are only using instrument as a target value, as there not shown to be benefit in training for model and manufacturer separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 766 ms, sys: 47.7 ms, total: 814 ms\n",
      "Wall time: 810 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_labelled = xbt_full_dataset.filter_obs({'labelled': 'labelled'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.get_ml_dataset(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xbt_labelled.filter_features(['instrument','model','manufacturer']).encode_target(return_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 4.01 ms, total: 32 ms\n",
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unseen_cruise_numbers = xbt_labelled.sample_feature_values('cruise_number', fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.24 s, sys: 0 ns, total: 8.24 s\n",
      "Wall time: 8.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xbt_unseen = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='include', check_type='in_filter_set')\n",
    "xbt_working = xbt_labelled.filter_obs({'cruise_number': unseen_cruise_numbers}, mode='exclude', check_type='in_filter_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_classes = xbt_labelled.xbt_df.apply(imeta_classification, axis=1)\n",
    "imeta_model = imeta_classes.apply(lambda t1: t1[0])\n",
    "imeta_manufacturer = imeta_classes.apply(lambda t1: t1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_instrument = imeta_classes.apply(lambda t1: f'XBT: {t1[0]} ({t1[1]})') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently training and evaulating separately for model and manufacturer. We will also need to train and evaulate together as this is ultimately what is wanted (a combined probe model and manufacturer field).\n",
    "\n",
    "We are using the default 80/20 split in scikit-learn for now. Further work will need to do proper cross validation where several different splits are randomly selected to verify our results are not an artifact of the randomly chosen split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xbt_train_all, xbt_test_all = xbt_working.train_test_split(refresh=True, features=['instrument', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = xbt_train_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_test_all = xbt_test_all.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "X_unseen_all = xbt_unseen.filter_features(input_feature_names).get_ml_dataset()[0]\n",
    "y_instr_train_all = xbt_train_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_instr_test_all = xbt_test_all.filter_features(['instrument']).get_ml_dataset()[0]\n",
    "y_instr_unseen_all = xbt_unseen.filter_features(['instrument']).get_ml_dataset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier\n",
    "\n",
    "We train the Decision tree classifier before and calculate F1, precision and recall for train, test and unseen sets, both for each class and averaged across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all = {}\n",
    "metrics_avg_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_dt_instr1 = classifier_class(**classifier_opts)\n",
    "clf_dt_instr1.fit(X_train_all,y_instr_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_class_all['instrument'] = list(xbt_labelled._feature_encoders['instrument'].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_train_instr_all = clf_dt_instr1.predict(X_train_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_train_all, y_res_train_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_train': metrics1[0],\n",
    "    'recall_instr_train': metrics1[1],\n",
    "    'f1_instr_train': metrics1[2],\n",
    "    'support_instr_train': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_train' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_train' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_train' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test_instr_all = clf_dt_instr1.predict(X_test_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_test_all, y_res_test_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_test': metrics1[0],\n",
    "    'recall_instr_test': metrics1[1],\n",
    "    'f1_instr_test': metrics1[2],\n",
    "    'support_instr_test': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_test' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_test' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_test' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_unseen_instr_all = clf_dt_instr1.predict(X_unseen_all)\n",
    "metrics1 = sklearn.metrics.precision_recall_fscore_support(y_instr_unseen_all, y_res_unseen_instr_all, labels=list(range(0,len(metrics_per_class_all['instrument']))))\n",
    "metrics_per_class_all.update( {\n",
    "    'precision_instr_unseen': metrics1[0],\n",
    "    'recall_instr_unseen': metrics1[1],\n",
    "    'f1_instr_unseen': metrics1[2],\n",
    "    'support_instr_unseen': metrics1[3],\n",
    "})\n",
    "metrics_avg_all.update({\n",
    "    'precision_instr_unseen' : sum(metrics1[0] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'recall_instr_unseen' : sum(metrics1[1] * metrics1[3])/ sum(metrics1[3]),\n",
    "    'f1_instr_unseen' : sum(metrics1[2] * metrics1[3])/ sum(metrics1[3]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_per_class_instr = pandas.DataFrame.from_dict({k1:v1 for k1,v1 in metrics_per_class_all.items() if 'instr' in k1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg = pandas.DataFrame.from_dict({\n",
    "    'target': ['instrument_train','instrument_test', 'instrument_unseen'],\n",
    "    'precision': [v1 for k1,v1 in metrics_avg_all.items() if 'precision' in k1],\n",
    "    'recall': [v1 for k1,v1 in metrics_avg_all.items() if 'recall' in k1],\n",
    "    'f1': [v1 for k1,v1 in metrics_avg_all.items() if 'f1' in k1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification result plots\n",
    "\n",
    "The plots below show the results for the whole XBT dataset. We see that the DT classifier performs well on the training data, but does not seem to generalise well. This especially true, as one would expect, for classes with very little support in the training dataset. Results are broadly comparable to using country as an input rather than lat/long. This suggests that if we are concerned about the problems with country code as an input feature, lat/long coordinate is a viable alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_results_all_dt = matplotlib.pyplot.figure('xbt_results_all_dt', figsize=(24,8))\n",
    "axis_instr_metrics = fig_results_all_dt.add_subplot(121)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument', y=['recall_instr_train','recall_instr_test','recall_instr_unseen'],ax=axis_instr_metrics)\n",
    "axis_instr_support = fig_results_all_dt.add_subplot(122)\n",
    "_ = df_metrics_per_class_instr.plot.bar(x='instrument',y=['support_instr_train', 'support_instr_test', 'support_instr_unseen'], ax=axis_instr_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_avg.plot.bar(figsize=(18,12), x='target', y='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "The contents of the XBT dataset varies over the time period, so previous papers have looked at classification accuracy (recall) year by year to evaluate how performance varies with different distribution of probe types.\n",
    "\n",
    "To do this we apply the classifier to the train and test data for each year separetly and calculate the metrics year by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_year(xbt_df, year, clf, input_features, target_feature):\n",
    "    X_year = xbt_df.filter_obs({'year': year}, ).filter_features(input_features).get_ml_dataset()[0]\n",
    "    y_year = xbt_df.filter_obs({'year': year} ).filter_features([target_feature]).get_ml_dataset()[0]\n",
    "    y_res_year = clf.predict(X_year)\n",
    "    metric_year = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_year, y_res_year, average='micro')\n",
    "    return metric_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress = ipywidgets.IntProgress(min=env_date_ranges[environment][0],\n",
    "                                           max= env_date_ranges[environment][1],\n",
    "                                          description='Evaluating',\n",
    "                                          bar_style='info')\n",
    "eval_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    results_by_year[year] = {\n",
    "        'metric_train_instr' : score_year(xbt_train_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_test_instr' : score_year(xbt_test_all, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "        'metric_unseen_instr' : score_year(xbt_unseen, year, clf_dt_instr1, input_feature_names, 'instrument'),\n",
    "    }\n",
    "    eval_progress.value = year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_by_year = pandas.DataFrame.from_dict({ \n",
    "    'year':  list(results_by_year.keys()),\n",
    "    'recall_train_instr' : [m1['metric_train_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_test_instr' : [m1['metric_test_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "    'recall_unseen_instr' : [m1['metric_unseen_instr'][1] for y1,m1 in results_by_year.items()],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr_encoder = xbt_labelled._feature_encoders['instrument']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_progress.value = env_date_ranges[environment][0]\n",
    "imeta_results = []\n",
    "for year in range(env_date_ranges[environment][0],env_date_ranges[environment][1]):\n",
    "    y_imeta_instr = instr_encoder.transform(pandas.DataFrame(imeta_instrument[xbt_labelled.xbt_df.year == year]))\n",
    "    xbt_instr1 = instr_encoder.transform(pandas.DataFrame(xbt_labelled.xbt_df[xbt_labelled.xbt_df.year == year].instrument))\n",
    "    (im_pr_instr, im_rec_instr, im_f1_instr, im_sup_instr) = sklearn.metrics.precision_recall_fscore_support(xbt_instr1, y_imeta_instr,average='micro')\n",
    "    imeta_results += [{'year': year,\n",
    "                       'imeta_instr_recall': im_rec_instr,\n",
    "                       'imeta_instr_precision': im_pr_instr,\n",
    "                      }]\n",
    "    eval_progress.value = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imeta_res_df = pandas.DataFrame.from_records(imeta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pandas.merge(recall_by_year, imeta_res_df).merge(\n",
    "    pandas.DataFrame.from_dict({\n",
    "        'year': xbt_labelled['year'].value_counts(sort=False).index,\n",
    "        'num_samples': xbt_labelled['year'].value_counts(sort=False).values,\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_model_recall_results = matplotlib.pyplot.figure('xbt_model_recall', figsize=(12,12))\n",
    "ax_instr_recall_results = fig_model_recall_results.add_subplot(111, title='XBT instrument recall results')\n",
    "_ = results_df.plot.line(x='year',y=['recall_train_instr','recall_test_instr', 'recall_unseen_instr', 'imeta_instr_recall'], ax=ax_instr_recall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for recall per year are again broadly comparable for this experiment where we use max depth, year and lat/long coordinate, compared to the original experiment where we used max depth, year and country code as the input features. This again supports lat/long coordinate as a viable alternative to country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['improvement_instr'] = results_df.apply(lambda r1: ((r1['recall_test_instr'] /  r1['imeta_instr_recall'])-1)*100.0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_num_samples_per_year = matplotlib.pyplot.figure('fig_num_samples_per_year', figsize=(16,8))\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(121, title='number of samples per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['num_samples'],c='purple' )\n",
    "ax_num_samples = fig_num_samples_per_year.add_subplot(122, title='improvement instrument per year')\n",
    "_ = results_df.plot.line(ax=ax_num_samples, x='year',y=['improvement_instr'], c='green' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(os.path.join(result_dir,result_fname_template.format(classifier=classifier_name,\n",
    "                                                                       suffix=suffix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing the data\n",
    "\n",
    "To filter based on what profiles we can use for predicting, we need some way of checking each profile we create these checkers from the labelled dataset, because the subset of data that was used for training determines what subset is valid for prediction. For example, if a country is not present in the training data, then the prediction function won't be able to handle that profile to predict a probe model and manufacturer. Profiles that are not handled by the trained classifier will get the label \"UNKNOWN\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checker functions check each element of the profile metadata that could be a problem. The checkers are constructed from the labelled data subset.\n",
    "checkers_labelled = {f1: c1 for f1, c1 in xbt_labelled.get_checkers().items() if f1 in input_feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_feature_name = 'instrument_res_dt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable = xbt_full_dataset.filter_predictable(checkers_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ml1 = clf_dt_instr1.predict(xbt_predictable.filter_features(input_feature_names).get_ml_dataset()[0])\n",
    "res2 = list(xbt_labelled._feature_encoders['instrument'].inverse_transform(res_ml1).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_predictable.xbt_df[result_feature_name] = res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.merge_features(xbt_predictable, [result_feature_name],\n",
    "                               fill_values = {result_feature_name: UNKNOWN_STR},\n",
    "                               feature_encoders={result_feature_name: xbt_labelled._feature_encoders['instrument']},\n",
    "                               target_encoders={result_feature_name: xbt_labelled._target_encoders['instrument']},\n",
    "                               output_formatters={result_feature_name: cat_output_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_full_dataset.output_data(os.path.join(output_dir, output_fname_template.format(classifier=classifier_name,\n",
    "                                                                                  suffix=suffix)),\n",
    "                             target_features=[result_feature_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We can see so far that the basic tree approach seems to be outperforming iMeta. Further work needs to be done to calculate these results more rigorously  using cross validation. \n",
    "\n",
    "The next step is to explore more sophisticated tree based approaches, such an ensemble of trees (random forest) and gredient-bossted tress (XGBoost),the current state of the art in tree methods.\n",
    "\n",
    "There are also some issues with the dataset currently being used, so these results may change when the correct version of the data is being used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XBT test 20200128",
   "language": "python",
   "name": "xbt_test_20200128"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
